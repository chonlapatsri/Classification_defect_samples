{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class classification with deep learning neural network\n",
    "The idea of this notebook is to demonstrate how to test different algorithms with hyperparameter tuning to train a model and how to evaluate a model that has two-level architecture which is a redacted version on my work on a very specific dataset on a totally unrelated topic.\n",
    "\n",
    "\n",
    "This the Deep Learning neural network for multiclass classification. In order to improve the overall performance of the model, hyperparameter tuning was carried out and at the same time earlystopping and callback were used to prevent overfitting and minimise training time. One Cycle learning rate scheduler technique was also used to get as close to the global optimal as possible.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Dataset Preparation steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  0\n",
       "0                5.1               3.5                1.4               0.2  0\n",
       "1                4.9               3.0                1.4               0.2  0\n",
       "2                4.7               3.2                1.3               0.2  0\n",
       "3                4.6               3.1                1.5               0.2  0\n",
       "4                5.0               3.6                1.4               0.2  0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download IRIS dataset for classification problem\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Display the DataFrame\n",
    "x = pd.DataFrame(iris.data, columns = iris.feature_names)\n",
    "y = pd.DataFrame(iris.target)\n",
    "\n",
    "df = pd.concat([x, y], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unwanted columns\n",
    "geo_param_to_remove = ['defect_1', 'defect_2', 'defect_3', 'defect_4', 'defect_5']\n",
    "for col in df.columns.tolist():\n",
    "    if 'PR' in col or 'NR' in col:\n",
    "        geo_param_to_remove.append(col)\n",
    "\n",
    "zone_info_to_remove = ['G_Mean', 'AUC', 'F-Measure', 'Specificity', 'Precision', 'Recall' \n",
    "                       , 'Youden', 'Balanced_Accuracy', 'ROC_Curve', 'PR_Curve', 'AP']\n",
    "\n",
    "remove_list = geo_param_to_remove + zone_info_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns = {0:'species'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)', 'species']\n"
     ]
    }
   ],
   "source": [
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we need to remove any unwanted columns\n",
    "param_to_remove = ['sepal length (cm)', 'sepal length (cm)']\n",
    "for col in df.columns.tolist():\n",
    "    if '(c' in col or 'm)' in col:\n",
    "        param_to_remove.append(col)\n",
    "\n",
    "others_to_remove = []\n",
    "\n",
    "remove_list = param_to_remove + others_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reload the DataFrame\n",
    "# Display the DataFrame\n",
    "x = pd.DataFrame(iris.data, columns = iris.feature_names)\n",
    "y = pd.DataFrame(iris.target)\n",
    "\n",
    "df = pd.concat([x, y], axis=1)\n",
    "df = df.rename(columns = {0:'species'})\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>species</th>\n",
       "      <th>Label_1</th>\n",
       "      <th>Label_2</th>\n",
       "      <th>Label_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "      <td>A2</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td>A2</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>D</td>\n",
       "      <td>C</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "4                5.0               3.6                1.4               0.2   \n",
       "\n",
       "   species Label_1 Label_2 Label_3  \n",
       "0        0       D       D      B2  \n",
       "1        0       C      A2       D  \n",
       "2        0       A      A2       C  \n",
       "3        0       D       C       D  \n",
       "4        0       B       C      B2  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adding three more random label columns\n",
    "num_rows = df.shape[0]\n",
    "\n",
    "new_cols = {'Label_1': np.random.choice(['A', 'B', 'C', 'D'], size=num_rows),\n",
    "            'Label_2': np.random.choice(['A2', 'A2', 'C', 'D'], size=num_rows),\n",
    "            'Label_3': np.random.choice(['B2', 'B', 'C', 'D'], size=num_rows)}\n",
    "\n",
    "df = df.assign(**new_cols)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More columns and rows filtering can be done here, in the original data there are others\n",
    "df[\"Label_2\"] = df.apply(lambda x: 'A' if 'A' in x[\"Label_2\"] else x[\"Label_2\"], axis=1)\n",
    "df[\"Label_3\"] = df.apply(lambda x: 'B' if 'B' in x[\"Label_3\"] else x[\"Label_3\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# check for null values, in this case there isn't any because it's a dummy dataset\n",
    "print(df.columns[df.isnull().any()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new column that collects every labels that exist in a row (except when it's A) which will be used for stratification\n",
    "key_types = ['species', 'Label_1', 'Label_2', 'Label_3']\n",
    "df[\"labels\"] = df.apply(lambda x: ','.join(x[x.index.isin(key_types)].index\n",
    "                                           [x[x.index.isin(key_types)] != 'A' ]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the df into final_test_set and global_train_set using defect combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: labels, dtype: int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find row with a unique combination of types that only have one sample which will create problem during split\n",
    "df[\"labels\"].value_counts().loc[lambda x: x<2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the row that have unique combination (we will add them to the test set), before dropping them from the main df\n",
    "unique_combination_ind = df[\"labels\"].value_counts().loc[lambda x: x<2].index\n",
    "unique_combination_df = df[df[\"labels\"].isin(unique_combination_ind)]\n",
    "\n",
    "#print(unique_combination_df)\n",
    "\n",
    "# drop rows with unique_combination\n",
    "#print(\"before drop: \", df.shape)\n",
    "df = df.drop(df[df[\"labels\"].isin(unique_combination_ind)].index).reset_index(drop=True)\n",
    "#print(\"after drop: \", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "# Defect_cat is 0 for without Niv or Nall, 1 for with Niv, Nall, or Both \n",
    "for train_index, test_index in split.split(df, df[\"labels\"]):\n",
    "    Global_train_set = df.loc[train_index]\n",
    "    Final_test_set_original = df.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "species,Label_1,Label_2,Label_3    12\n",
       "species,Label_1,Label_3            10\n",
       "species,Label_2,Label_3             5\n",
       "species,Label_3                     3\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the unique combination rows to the Final_test_set\n",
    "Final_test_set_original = pd.concat([Final_test_set_original, unique_combination_df], ignore_index=True)\n",
    "\n",
    "Final_test_set_original[\"labels\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the labels colmun from test and train sets\n",
    "Global_train_set = Global_train_set.drop(\"labels\", axis=1)\n",
    "Final_test_set_original = Final_test_set_original.drop(\"labels\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>species</th>\n",
       "      <th>Label_1</th>\n",
       "      <th>Label_2</th>\n",
       "      <th>Label_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>6.6</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>6.2</td>\n",
       "      <td>2.2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>6.2</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "58                 6.6               2.9                4.6               1.3   \n",
       "138                6.0               3.0                4.8               1.8   \n",
       "68                 6.2               2.2                4.5               1.5   \n",
       "126                6.2               2.8                4.8               1.8   \n",
       "147                6.5               3.0                5.2               2.0   \n",
       "\n",
       "     species Label_1 Label_2 Label_3  \n",
       "58         1       B       C       B  \n",
       "138        2       B       D       D  \n",
       "68         1       D       A       D  \n",
       "126        2       D       A       B  \n",
       "147        2       B       A       D  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Global_train_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End final test set split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Multiclass training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Global_train_set.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find if there is any column that is not numeric \n",
    "df_cat = df.select_dtypes(exclude=[np.number])\n",
    "#df_cat.head()\n",
    "\n",
    "# Add categorical columns that are number manually can convert them to categorical type\n",
    "other_cat_col = []\n",
    "df[other_cat_col] = df[other_cat_col].astype(\"category\")\n",
    "#df[other_cat_col].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "ordinal_encoder = OrdinalEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat_encoded = ordinal_encoder.fit_transform(df_cat)\n",
    "#print(df_cat_encoded)\n",
    "\n",
    "df[df_cat.columns.tolist()] = df_cat_encoded\n",
    "df[df_cat.columns.tolist()] = df[df_cat.columns.tolist()].astype(\"category\")\n",
    "#df[df_cat.columns.tolist()].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "end of step 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train/Val split (normal split here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df[\"species\"]\n",
    "X_train = df.drop(\"species\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "end of step 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Visualisation on train set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Oversampling the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    35\n",
       "2    31\n",
       "0    30\n",
       "Name: species, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.364583\n",
       "2    0.322917\n",
       "0    0.312500\n",
       "Name: species, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We check the ratio of the labels\n",
    "y_train.value_counts() / len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the size of class is too different, we can use oversampling below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before oversampling Counter({1: 35, 2: 31, 0: 30})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "counter = Counter(y_train)\n",
    "print('Before oversampling', counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampling = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "if oversampling:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    smote = SMOTE(sampling_strategy='not majority', random_state = 42) # sampling_strategy = float only works for Binary \n",
    "    \n",
    "    X_train_pre_transf, y_train_pre_transf = smote.fit_resample(X_train, y_train)\n",
    "    counter = Counter(y_train_pre_transf)\n",
    "    print('After', counter)\n",
    "else:\n",
    "    X_train_pre_transf = X_train\n",
    "    y_train_pre_transf = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>Label_1</th>\n",
       "      <th>Label_2</th>\n",
       "      <th>Label_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>7.3</td>\n",
       "      <td>2.9</td>\n",
       "      <td>6.3</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>6.8</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "107                7.3               2.9                6.3               1.8   \n",
       "76                 6.8               2.8                4.8               1.4   \n",
       "110                6.5               3.2                5.1               2.0   \n",
       "11                 4.8               3.4                1.6               0.2   \n",
       "28                 5.2               3.4                1.4               0.2   \n",
       "\n",
       "    Label_1 Label_2 Label_3  \n",
       "107     0.0     2.0     0.0  \n",
       "76      0.0     2.0     0.0  \n",
       "110     2.0     0.0     2.0  \n",
       "11      0.0     2.0     1.0  \n",
       "28      1.0     0.0     2.0  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(X_train_pre_transf[df_cat.columns.tolist()].value_counts())\n",
    "X_train_pre_transf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "end of step 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_set = X_train_pre_transf.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_attribs = X_train_set.select_dtypes(include='category').columns.tolist()\n",
    "#print(cat_attribs)\n",
    "\n",
    "X_train_num_lst = X_train_set.drop(cat_attribs, axis=1).columns.tolist()\n",
    "X_train_cat_lst = X_train_set[cat_attribs].columns.tolist()\n",
    "\n",
    "#print(X_train_num_lst)\n",
    "#print(X_train_cat_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a transforamtion class to add, combine, fuse, extract features for the numeric PL\n",
    "# stat_feature_head_to_tail is to use only the absolute 'ab_maximum' or 'average' value of each statistical features  \n",
    "class Attrib_transformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, stat_features = 'ab_maximum'):\n",
    "        self.stat_features = stat_features\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        if self.stat_features is not None:\n",
    "            stat_feature = ['sepal', 'petal']\n",
    "            measurement = ['length', 'width']\n",
    "            \n",
    "            new_X = X.copy() # so the original df is not changed\n",
    "            # See the commnad line argument and match it to pre-defined case \n",
    "            match self.stat_features:\n",
    "                case 'ab_maximum':\n",
    "                    for stat in stat_features:\n",
    "                        meaurement_list= []\n",
    "                        for measure in measurement:                            \n",
    "                            # create a list of all measurement of each feature\n",
    "                            temp = stat + ' ' + measure + ' (cm)'\n",
    "                            meaurement_list.append(temp)\n",
    "                            \n",
    "                        \n",
    "                        # Get the absolute maximum value of each row, keep the sign, and put them in a new colmun\n",
    "                        col_name = measure + '_' + self.stat_features\n",
    "                        row_max = X[meaurement_list].abs().max(axis=1)\n",
    "                        new_X[col_name] = X[meaurement_list].max(axis=1).mask(lambda x: x < row_max, -row_max)\n",
    "                        new_X = new_X.drop(meaurement_list, axis=1)\n",
    "                \n",
    "                case 'average':\n",
    "                    for stat in stat_feature:\n",
    "                        meaurement_list= []\n",
    "                        for measure in measurement:                            \n",
    "                            # create a list of all measurement of each feature\n",
    "                            temp = stat + ' ' + measure + ' (cm)'\n",
    "                            meaurement_list.append(temp)\n",
    "                            \n",
    "                        # Get the mean value of each row and put them in a new colmun \n",
    "                        col_name = measure + '_' + self.stat_features\n",
    "                        new_X[col_name] = X[meaurement_list].mean(axis=1)\n",
    "                        new_X = new_X.drop(meaurement_list, axis=1)\n",
    "                    \n",
    "                case _:   # 'case _' is for any other input that does not match None or the above\n",
    "                    sys.exit(\"please choose from 'ab_maximum', 'average', or \\\n",
    "                    'None' for stat_featurs in Attrib_transformer\")\n",
    "                \n",
    "            #print(list(new_X.columns))\n",
    "            return new_X\n",
    "            \n",
    "        else:\n",
    "            return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a transforamtion class to drop unwanted features\n",
    "class Attrib_drop(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, d_list=[], keep_only_length=False):\n",
    "        self.d_list = d_list\n",
    "        self.keep_only_length = keep_only_length\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        if self.keep_only_length:\n",
    "            Vert_acc_indices = []\n",
    "            for i, col in enumerate(X.columns):\n",
    "                if 'length' in col:\n",
    "                    length_indices.append(i)\n",
    "            return X.iloc[:,length_indices]\n",
    "        elif self.d_list:\n",
    "            keep_indices = []\n",
    "            for i, col in enumerate(X.columns):\n",
    "                if col not in self.d_list:\n",
    "                    keep_indices.append(i)\n",
    "            return X.iloc[:,keep_indices]\n",
    "        else:\n",
    "            return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of the numerical features to be dropped to manually drop columns from the df\n",
    "drop_list_num = [] # Here we can add columns that should be dropped from the df\n",
    "\n",
    "num_pl = Pipeline([\n",
    "    #('imputer', SimpleImputer(strategy=\"median\")),\n",
    "    ('attrib_transform', Attrib_transformer(stat_features= None)),\n",
    "    ('attrib_drop', Attrib_drop(keep_only_length=False, d_list=drop_list_num)),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "#For testing num_pl\n",
    "#num_prep = num_pl.fit_transform(X_train_set[X_train_num_lst])\n",
    "#print('shape after transformation: ', num_prep.shape)\n",
    "#num_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of the categorical features to be dropped to manually drop columns from the df\n",
    "drop_list_cat = ['Lane']\n",
    "\n",
    "cat_pl = Pipeline([\n",
    "    #('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('attrib_drop', Attrib_drop(d_list=drop_list_cat)),\n",
    "    ('One_Hot', OneHotEncoder(sparse_output=False)),\n",
    "])\n",
    "\n",
    "# For testing cat_pl\n",
    "#cat_prep = cat_pl.fit_transform(X_train_set[X_train_cat_lst])\n",
    "#cat_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "full_pl = ColumnTransformer([\n",
    "    (\"num\", num_pl, X_train_num_lst),\n",
    "    (\"cat\", cat_pl, X_train_cat_lst),\n",
    "])\n",
    "\n",
    "X_train_prep = full_pl.fit_transform(X_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape after transformation:  (96, 14)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.72649478, -0.31247962,  1.45414628, ...,  1.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 1.12379115, -0.52985674,  0.58886915, ...,  1.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.76216897,  0.33965176,  0.76192458, ...,  0.        ,\n",
       "         0.        ,  1.        ],\n",
       "       ...,\n",
       "       [ 0.03892461, -0.09510249,  0.2427583 , ...,  0.        ,\n",
       "         0.        ,  1.        ],\n",
       "       [ 0.03892461,  0.33965176,  0.58886915, ...,  1.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.15946534, -0.74723387,  0.76192458, ...,  0.        ,\n",
       "         0.        ,  1.        ]])"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('shape after transformation: ', X_train_prep.shape)\n",
    "X_train_prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 OPTIONAL: PCA for unsupervised feature selectiion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of components:  7\n"
     ]
    }
   ],
   "source": [
    "use_PCA = True\n",
    "\n",
    "if use_PCA:\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(copy=True, iterated_power='auto', n_components=0.9, random_state=None,\n",
    "              svd_solver='auto', tol=0.0, whiten=False)\n",
    "    \n",
    "    X_train_prep = pca.fit_transform(X_train_prep)\n",
    "    print('number of components: ', pca.n_components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape after transformation:  (96, 7)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2.36128977e+00,  3.81729732e-01, -8.19431893e-01,\n",
       "        -5.60556471e-01, -6.18922296e-01,  1.62915139e-01,\n",
       "        -4.01008096e-01],\n",
       "       [ 1.29273455e+00, -6.11329349e-02, -8.59429405e-01,\n",
       "        -5.91388901e-01, -7.07990536e-01,  1.45654790e-01,\n",
       "        -3.64838831e-01],\n",
       "       [ 1.27357508e+00,  5.93886968e-01,  9.47539922e-01,\n",
       "         7.82955625e-01,  3.78745314e-01, -1.08406051e-01,\n",
       "        -6.42101954e-01],\n",
       "       [-2.30429507e+00,  1.70595380e-01,  2.23995699e-03,\n",
       "        -9.27626571e-01, -9.54926848e-01,  9.39990211e-01,\n",
       "        -1.31813891e-01],\n",
       "       [-2.23598129e+00,  4.42617537e-01,  7.48386994e-01,\n",
       "         2.72750929e-01, -2.99530606e-01, -8.28689207e-01,\n",
       "         3.68649226e-01],\n",
       "       [ 9.49200544e-02, -1.16636680e+00, -5.70432916e-01,\n",
       "         7.07225371e-01, -3.37982130e-01, -1.57285639e-01,\n",
       "         5.38262006e-01],\n",
       "       [ 1.89991262e+00,  7.65736607e-01,  1.08890206e+00,\n",
       "        -5.84771030e-01,  3.98528264e-01,  4.48433387e-01,\n",
       "        -2.01618777e-01],\n",
       "       [-2.42328350e+00, -2.44997291e-01,  2.21629148e-01,\n",
       "         3.50318562e-01, -9.72273652e-01,  8.26759133e-01,\n",
       "         2.25184602e-01],\n",
       "       [-2.14998546e+00,  1.10368683e+00, -8.28256540e-01,\n",
       "        -8.52687572e-01, -1.92235586e-01, -6.66418670e-01,\n",
       "         4.13793037e-02],\n",
       "       [-1.12394985e-01, -1.07058861e+00,  9.15561070e-01,\n",
       "        -2.45767887e-01,  1.09783433e+00,  2.39899875e-01,\n",
       "        -3.29734890e-01],\n",
       "       [-2.38030278e+00,  4.38928774e-01,  7.91789150e-01,\n",
       "         6.49696236e-01,  1.66583850e-01, -2.84441617e-02,\n",
       "        -5.80887521e-01],\n",
       "       [ 1.09484003e+00, -1.71926988e+00,  7.54127211e-01,\n",
       "        -9.10480542e-02,  9.96439890e-01,  1.90460159e-01,\n",
       "        -1.21289240e-01],\n",
       "       [ 8.00551000e-01,  5.85858417e-01, -7.65929946e-01,\n",
       "        -5.39294995e-01, -2.16515387e-01,  1.07947717e-02,\n",
       "         1.08860937e-01],\n",
       "       [-2.78038580e+00,  4.28822217e-01, -7.25802557e-01,\n",
       "        -5.36722716e-01,  2.57582378e-01,  9.92755756e-02,\n",
       "        -9.40055062e-01],\n",
       "       [ 1.86267212e+00,  6.56707320e-01,  3.17843601e-01,\n",
       "         7.02129420e-01, -3.37045767e-01,  5.79934738e-01,\n",
       "         8.04739764e-01],\n",
       "       [ 2.10542910e+00,  8.40304169e-01,  7.40087456e-01,\n",
       "        -8.16411475e-01, -5.64290057e-01,  1.17556170e-01,\n",
       "        -6.51477542e-01],\n",
       "       [-2.63946597e+00,  1.64074576e+00,  1.37995071e-01,\n",
       "        -2.19351102e-01,  9.32129083e-01,  1.22687993e+00,\n",
       "         1.02286392e-01],\n",
       "       [ 2.58390655e-02, -2.37255560e-01,  6.42085618e-01,\n",
       "        -5.37794403e-01,  2.34840518e-01, -2.54989488e-02,\n",
       "        -8.93000433e-01],\n",
       "       [-2.24986849e+00,  2.68403709e+00,  8.15308166e-01,\n",
       "         2.36226924e-01, -1.27007822e-01, -6.95140417e-01,\n",
       "         1.93949744e-01],\n",
       "       [ 2.46202807e+00,  2.46617705e+00, -7.82496574e-01,\n",
       "        -5.72113391e-01, -4.72317359e-01,  2.83518017e-01,\n",
       "        -5.37013554e-01],\n",
       "       [ 1.92518211e+00,  9.12552405e-01,  1.04962796e+00,\n",
       "         7.71873300e-01,  4.76726787e-01, -1.17106735e-01,\n",
       "        -7.34099608e-01],\n",
       "       [ 5.89625061e-01, -3.11596354e-01, -5.22924246e-01,\n",
       "        -1.08964086e-02,  1.19164884e+00,  3.17474915e-01,\n",
       "        -2.35382036e-01],\n",
       "       [ 1.23917178e+00, -8.77814322e-01, -7.73922470e-01,\n",
       "        -6.42119633e-01, -7.02981103e-01,  9.26849071e-02,\n",
       "        -4.03370802e-01],\n",
       "       [ 1.28625363e+00,  4.05740618e-01,  8.86498158e-01,\n",
       "        -5.62299693e-01,  7.67406520e-01, -4.88473320e-01,\n",
       "         5.60882972e-01],\n",
       "       [ 1.80166560e+00,  3.68040030e-01,  9.29820840e-01,\n",
       "        -5.30338897e-01,  7.91231193e-01, -5.39759454e-01,\n",
       "         5.42926471e-01],\n",
       "       [ 2.88886549e-01, -1.04625863e+00, -6.92318781e-01,\n",
       "        -7.51914562e-01, -6.83286290e-01,  1.10475275e-01,\n",
       "        -4.95216022e-01],\n",
       "       [-2.09332347e+00,  1.54729730e+00, -5.26340180e-01,\n",
       "         3.85432362e-01, -9.88178315e-02, -7.27841426e-01,\n",
       "         2.82140583e-01],\n",
       "       [-2.31449577e+00,  2.31701106e-01, -5.84657160e-01,\n",
       "         8.08817547e-01,  2.27080288e-01,  5.40372789e-03,\n",
       "        -5.11124027e-01],\n",
       "       [ 1.97748277e+00,  9.83268846e-01, -5.87900855e-01,\n",
       "         8.21708476e-01, -1.64073061e-01, -5.44659119e-02,\n",
       "         4.89322470e-01],\n",
       "       [-2.16316001e+00,  1.50756255e+00,  7.99703300e-01,\n",
       "         2.96062490e-01, -7.34954027e-01,  1.78597952e-01,\n",
       "        -3.05669900e-01],\n",
       "       [ 2.65862526e+00,  8.02278262e-01,  7.97986433e-01,\n",
       "         5.27815799e-01, -9.99249482e-02, -9.02679569e-01,\n",
       "         4.21561498e-01],\n",
       "       [ 1.89229489e+00,  4.12434229e-01, -4.31048216e-01,\n",
       "         6.16655160e-01, -5.21802710e-01,  5.75540577e-02,\n",
       "        -2.26209697e-01],\n",
       "       [ 1.57275922e+00, -8.53332988e-01, -6.78605637e-01,\n",
       "        -6.63493314e-01, -6.38880788e-01,  6.61445072e-02,\n",
       "        -4.80601000e-01],\n",
       "       [ 4.98970458e-01, -2.35640025e-01, -6.11053060e-01,\n",
       "         3.10381436e-02,  1.13848858e+00,  3.30345244e-01,\n",
       "        -1.60982274e-01],\n",
       "       [ 2.71814228e-02, -1.41132744e+00,  7.80966397e-01,\n",
       "         5.57405756e-01, -4.38046088e-01, -2.11523118e-01,\n",
       "         5.13008954e-01],\n",
       "       [ 1.24596529e+00, -1.11120806e+00,  8.03158259e-01,\n",
       "        -9.04103595e-01, -6.62368406e-01,  1.51190691e-02,\n",
       "        -6.32068312e-01],\n",
       "       [-3.72955652e-01, -2.56169233e+00,  2.45608099e-01,\n",
       "         8.22956578e-01, -5.92314145e-02,  4.89554550e-01,\n",
       "         3.95139441e-02],\n",
       "       [-2.10008153e+00,  1.17368034e+00,  8.56513269e-01,\n",
       "         2.55847135e-01, -7.08535910e-01,  1.71649803e-01,\n",
       "        -3.43020720e-01],\n",
       "       [-2.09852799e+00,  1.90608485e-01, -5.92490575e-01,\n",
       "        -1.28203321e-01,  1.09658006e+00,  4.04210060e-01,\n",
       "        -2.61175582e-01],\n",
       "       [-1.98646301e+00,  5.30261949e-01, -6.29094820e-01,\n",
       "         4.45746285e-01, -2.18640666e-01, -7.83932293e-01,\n",
       "         4.29612623e-01],\n",
       "       [-2.01446683e+00, -2.09893709e+00,  5.48916555e-01,\n",
       "        -9.96153041e-01, -4.48872847e-01, -9.20844892e-01,\n",
       "         1.49463327e-01],\n",
       "       [ 2.10137796e+00,  8.08413777e-01,  7.18832035e-01,\n",
       "        -6.78760188e-01, -1.66782704e-01, -5.44554305e-02,\n",
       "        -2.82997382e-02],\n",
       "       [-2.18506132e+00,  1.73637624e+00, -7.29864914e-01,\n",
       "        -5.48547166e-02,  1.09586961e+00,  5.04485341e-01,\n",
       "        -2.14674863e-01],\n",
       "       [ 1.48868572e+00, -4.39932011e-01,  9.13346889e-01,\n",
       "        -9.40570125e-01, -5.52619031e-01,  3.18146806e-02,\n",
       "        -7.58286774e-01],\n",
       "       [ 1.14752670e+00, -2.58129200e-01, -4.68464212e-01,\n",
       "         7.22995463e-01, -1.95841740e-01, -1.42435603e-01,\n",
       "         4.27172317e-01],\n",
       "       [ 1.69690113e-01, -2.28163233e-01,  8.01023590e-01,\n",
       "        -1.02178455e+00, -1.05779383e-01, -8.42004085e-01,\n",
       "        -1.27973554e-01],\n",
       "       [ 2.36605424e+00,  1.80004141e+00,  7.39189852e-01,\n",
       "        -6.71042066e-01, -9.02122472e-02, -4.54642156e-03,\n",
       "        -8.97410781e-02],\n",
       "       [-2.30666332e+00, -5.96823492e-01,  1.15109055e-01,\n",
       "         5.43919194e-01, -6.45156007e-01,  6.34191304e-01,\n",
       "         9.46176883e-01],\n",
       "       [-2.53507101e+00,  1.05862065e-01,  2.43598484e-01,\n",
       "        -6.69390480e-01,  4.61675697e-01,  3.08895299e-01,\n",
       "         9.95338869e-01],\n",
       "       [ 9.37273385e-01, -1.25096315e+00, -8.40848480e-01,\n",
       "        -6.60197907e-01, -2.50166226e-01, -8.80662710e-01,\n",
       "         2.47684548e-01],\n",
       "       [ 1.18783020e-01, -6.95474742e-01, -5.61054695e-01,\n",
       "         5.66033031e-01, -7.09426593e-01,  5.79154524e-02,\n",
       "        -9.90162596e-02],\n",
       "       [-2.24013983e+00, -8.70622916e-02,  7.47653923e-01,\n",
       "         3.17493646e-01, -8.61827750e-01,  8.82851741e-02,\n",
       "        -1.79958682e-01],\n",
       "       [ 2.24874582e+00,  4.90407470e-01, -5.36023503e-01,\n",
       "        -2.67957416e-01,  3.03370247e-01,  4.78993652e-01,\n",
       "         1.07452763e-01],\n",
       "       [ 1.02779538e+00,  7.77663854e-01, -6.39965686e-01,\n",
       "         1.03779401e+00,  3.20860894e-01, -3.00695330e-02,\n",
       "        -4.06767788e-01],\n",
       "       [-2.49792052e+00, -8.65283195e-01, -7.49691120e-01,\n",
       "        -5.09224034e-01,  1.76400000e-01,  1.80015721e-02,\n",
       "        -8.46554051e-01],\n",
       "       [-6.59608734e-01, -1.77547501e+00, -4.56169350e-01,\n",
       "         8.36937162e-01,  2.38340397e-01, -1.61508679e-01,\n",
       "        -4.87021585e-01],\n",
       "       [-2.19177223e+00,  5.42288598e-01, -8.52196708e-01,\n",
       "        -6.69397896e-01, -3.60397291e-01,  7.68114378e-02,\n",
       "         1.17441038e-01],\n",
       "       [ 1.89086929e-01, -3.19464157e-01,  8.97915075e-01,\n",
       "         3.69018614e-01, -7.03525242e-01,  2.80749575e-02,\n",
       "        -2.49818476e-01],\n",
       "       [-2.22811502e+00, -6.16084742e-01,  7.33098917e-01,\n",
       "         6.89042883e-01,  7.33411499e-02, -9.09701934e-02,\n",
       "        -4.70615362e-01],\n",
       "       [ 1.33957593e-01, -8.48142619e-01,  8.79456716e-01,\n",
       "         7.42359195e-01,  2.24899830e-01, -1.54645756e-01,\n",
       "        -5.37491476e-01],\n",
       "       [ 2.91666737e+00,  3.57224823e-01, -7.22739066e-01,\n",
       "        -5.86598305e-03,  6.13960692e-01,  3.04052380e-01,\n",
       "         9.01869426e-01],\n",
       "       [ 1.01809346e+00, -8.03873358e-01,  3.58768214e-01,\n",
       "        -1.13823963e-01,  1.02815490e+00,  9.71988018e-01,\n",
       "         1.15286277e-01],\n",
       "       [-2.69546049e+00, -7.92045220e-01, -7.31419570e-01,\n",
       "        -9.38353612e-01, -2.57860112e-01, -7.76694718e-01,\n",
       "         3.52172875e-02],\n",
       "       [ 1.52388096e-01, -7.08879977e-01,  2.43227717e-01,\n",
       "         4.90384096e-01, -9.10528834e-01,  7.42993806e-01,\n",
       "         2.91161708e-01],\n",
       "       [-7.59763579e-02, -1.49642057e+00,  1.14914987e-01,\n",
       "        -2.83888368e-01,  2.41267381e-01,  9.45062665e-01,\n",
       "         1.27970033e+00],\n",
       "       [-1.92589429e+00,  5.42711212e-01,  7.59478367e-01,\n",
       "         2.91520456e-01, -2.81846731e-01, -8.38893932e-01,\n",
       "         3.62835111e-01],\n",
       "       [ 2.31325911e-01, -1.69531369e+00, -4.96326805e-01,\n",
       "         9.15036543e-01,  2.41441651e-01, -1.78032839e-01,\n",
       "        -4.31706948e-01],\n",
       "       [ 3.86359993e-01, -4.80035283e-01, -6.09351061e-01,\n",
       "         2.87880429e-02,  1.11850723e+00,  3.10002174e-01,\n",
       "        -1.51359635e-01],\n",
       "       [ 1.94163102e+00,  5.04898982e-01, -3.43887794e-01,\n",
       "         9.43696612e-01,  5.09215757e-01, -1.02502345e-01,\n",
       "        -6.37940022e-01],\n",
       "       [ 2.49712118e-01, -9.96487630e-01, -7.71240785e-01,\n",
       "        -3.35212852e-01,  2.35330100e-01, -4.66868934e-02,\n",
       "        -7.53669001e-01],\n",
       "       [ 1.53263611e+00,  6.91311683e-01, -2.78810626e-01,\n",
       "         4.75543519e-01,  1.05527697e-01, -8.87084966e-01,\n",
       "         1.89855680e-01],\n",
       "       [ 1.04008689e+00, -2.58634727e-01, -5.51998172e-01,\n",
       "        -3.82487478e-01,  7.49549939e-01, -4.80075463e-01,\n",
       "         7.04534376e-01],\n",
       "       [ 5.59388676e-01, -3.77405685e-01,  2.13741218e-01,\n",
       "         5.18060038e-01, -8.89740153e-01,  7.74199379e-01,\n",
       "         3.07626775e-01],\n",
       "       [-2.64831466e+00, -4.87124076e-01,  7.90965599e-01,\n",
       "        -3.30738112e-01,  9.81646948e-01,  3.41100445e-01,\n",
       "        -3.07522462e-01],\n",
       "       [ 1.06157994e+00, -2.12610009e-01, -2.21739586e-02,\n",
       "        -7.06774640e-01, -9.00196518e-01,  8.34502705e-01,\n",
       "        -6.04662861e-03],\n",
       "       [ 8.74943324e-01,  5.36277958e-02,  7.64273841e-01,\n",
       "        -9.50117468e-01, -9.82740587e-02, -8.51413456e-01,\n",
       "        -8.86121308e-02],\n",
       "       [ 7.96549321e-01,  5.68356412e-01, -5.73976053e-01,\n",
       "         5.84551378e-01, -1.03428519e-01, -8.49811384e-01,\n",
       "         4.45934470e-01],\n",
       "       [-4.91149939e-01, -1.43791366e+00, -5.50608469e-01,\n",
       "        -2.85360277e-01,  5.07564902e-01,  2.32832653e-01,\n",
       "         7.57130767e-01],\n",
       "       [ 1.10774416e+00, -1.48197503e+00,  7.34164378e-01,\n",
       "         6.55352719e-01, -4.45680581e-01, -2.48061458e-01,\n",
       "         5.88083026e-01],\n",
       "       [-9.45586735e-02, -9.39354018e-01, -5.65207725e-01,\n",
       "         5.66469948e-01, -7.39523685e-01,  3.23743638e-02,\n",
       "        -8.49181015e-02],\n",
       "       [-1.84208812e+00,  1.86157376e-01, -7.42872645e-01,\n",
       "        -8.74633793e-01, -1.87434004e-01, -7.43638758e-01,\n",
       "         1.89537895e-02],\n",
       "       [ 1.36448795e-01, -1.17040340e+00, -4.47153230e-01,\n",
       "         4.73918252e-01, -1.43135199e-01, -9.47141521e-01,\n",
       "         4.01078429e-01],\n",
       "       [-2.41056444e+00,  7.03536105e-01, -5.93902931e-01,\n",
       "        -5.57291263e-01,  6.74895184e-01, -3.54227432e-01,\n",
       "         6.12462654e-01],\n",
       "       [ 1.46066958e+00,  1.82712548e-01,  2.60277756e-01,\n",
       "        -4.05930106e-01,  6.77703954e-02,  1.18935059e+00,\n",
       "         4.94646647e-01],\n",
       "       [ 2.80508091e+00,  8.31877549e-01, -5.85528762e-01,\n",
       "         8.90005769e-01, -1.60704698e-01, -1.10667697e-01,\n",
       "         5.20866087e-01],\n",
       "       [ 6.12643461e-01, -2.20886881e-01, -5.04665954e-01,\n",
       "         9.33270042e-01,  3.39420792e-01, -8.21020964e-02,\n",
       "        -4.91967137e-01],\n",
       "       [-2.25124341e+00,  2.75670608e-01, -6.03428601e-01,\n",
       "         8.23769648e-01,  2.19054169e-01,  6.50329689e-03,\n",
       "        -4.94350575e-01],\n",
       "       [-2.31661235e+00, -5.05222214e-01,  7.16982419e-01,\n",
       "        -5.06478253e-01,  3.81001810e-01,  3.11872254e-01,\n",
       "         7.02636962e-01],\n",
       "       [-2.40537171e+00,  2.12842721e+00,  1.41623204e-01,\n",
       "         5.23284598e-01, -4.71456024e-01,  8.03365789e-01,\n",
       "         7.81609589e-01],\n",
       "       [ 1.87157482e+00,  5.13582484e-01, -1.28314029e-01,\n",
       "        -4.83612734e-01, -4.82213428e-01,  7.09637081e-01,\n",
       "         6.73624659e-01],\n",
       "       [ 1.67892354e+00,  9.97350443e-01, -4.93189636e-01,\n",
       "        -6.30204979e-01,  1.02414355e-03, -6.76884563e-03,\n",
       "        -1.38575200e-01],\n",
       "       [ 8.71424169e-01,  2.83243836e-01, -5.78362800e-01,\n",
       "         6.32439868e-01, -6.49116228e-01,  8.57733931e-02,\n",
       "        -1.10804239e-01],\n",
       "       [ 9.68689525e-01,  7.41437266e-02, -4.25905950e-01,\n",
       "        -4.45988810e-01,  8.40188105e-01, -4.78853791e-01,\n",
       "         5.73326239e-01],\n",
       "       [ 3.19600491e-01, -1.01082235e-01,  9.03468208e-01,\n",
       "         7.46931545e-01,  2.84860851e-01, -1.23347143e-01,\n",
       "        -5.90802016e-01],\n",
       "       [ 6.65606783e-01,  3.18480660e-01, -3.38163001e-01,\n",
       "         8.51947207e-01,  4.73248527e-01, -6.87051918e-02,\n",
       "        -6.68505955e-01],\n",
       "       [ 9.54423195e-01, -5.85543030e-01,  8.62236989e-01,\n",
       "        -5.68226471e-01,  6.84092235e-01, -5.41847675e-01,\n",
       "         6.25307991e-01]])"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('shape after transformation: ', X_train_prep.shape)\n",
    "X_train_prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Transform X_val as well using the pipeline that has been fitted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of components:  7\n",
      "shape after transformation:  (24, 7)\n"
     ]
    }
   ],
   "source": [
    "#Transform the X_test as the train set has been transformed\n",
    "X_val_prep = full_pl.transform(X_val)\n",
    "#print('shape before PCA: ', X_train_prep.shape)\n",
    "\n",
    "if use_PCA:\n",
    "    X_val_prep = pca.transform(X_val_prep)\n",
    "    print('number of components: ', pca.n_components_)\n",
    "\n",
    "print('shape after transformation: ', X_val_prep.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Train NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from hyperopt import hp, tpe, fmin, STATUS_OK, Trials, space_eval\n",
    "from hyperopt.early_stop import no_progress_loss\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a class for onecycle learning rate rescheduler\n",
    "K = keras.backend\n",
    "\n",
    "class OneCycleScheduler(keras.callbacks.Callback):\n",
    "    def __init__(self, iterations, max_rate, start_rate=None,\n",
    "                 last_iterations=None, last_rate=None):\n",
    "        self.iterations = iterations\n",
    "        self.max_rate = max_rate\n",
    "        self.start_rate = start_rate or max_rate / 10\n",
    "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "        self.last_rate = last_rate or self.start_rate / 1000\n",
    "        self.iteration = 0\n",
    "    def _interpolate(self, iter1, iter2, rate1, rate2):\n",
    "        return ((rate2 - rate1) * (self.iteration - iter1)\n",
    "                / (iter2 - iter1) + rate1)\n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self.iteration < self.half_iteration:\n",
    "            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n",
    "                                     self.max_rate, self.start_rate)\n",
    "        else:\n",
    "            rate = self._interpolate(2 * self.half_iteration, self.iterations,\n",
    "                                     self.start_rate, self.last_rate)\n",
    "        self.iteration += 1\n",
    "        K.set_value(self.model.optimizer.learning_rate, rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_train_prep.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_hidden, n_neurons, rho, dropout_rate, inputshape=input_size):\n",
    "    \"\"\"\n",
    "    Function to build the neural network model based on the provided hyperparameters.\n",
    "    \n",
    "    Args:\n",
    "        params (dict): Dictionary containing the hyperparameters.\n",
    "        inputshape (int): Number of input features.\n",
    "    \n",
    "    Returns:\n",
    "        model (Sequential): Compiled Keras model.\n",
    "    \"\"\"\n",
    "    #print(n_hidden, n_neurons, rho, dropout_rate)\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(n_neurons, activation=\"selu\", kernel_initializer=\"lecun_normal\", input_shape=(inputshape,))\n",
    "    ])\n",
    "\n",
    "    for _ in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"selu\", kernel_initializer=\"lecun_normal\"))\n",
    "        model.add(keras.layers.AlphaDropout(rate=dropout_rate))  # Add AlphaDropout layer with specified dropout rate\n",
    "\n",
    "    model.add(keras.layers.Dense(3, activation=\"softmax\"))\n",
    "    optimizer = keras.optimizers.RMSprop(rho=rho)\n",
    "\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    \"\"\"\n",
    "    Objective function for hyperparameter optimization. Calculates the loss value (1 - accuracy) based on cross-validation.\n",
    "    \n",
    "    Args:\n",
    "        params (dict): Dictionary containing the hyperparameters.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing the loss value, status, and cross-validation score.\n",
    "    \"\"\" \n",
    "    mod_params = params.copy()\n",
    "    mod_params.pop('max_lr')\n",
    "    mod_params.pop('batch_size')\n",
    "    n_epochs = 1000\n",
    "    model = build_model(**mod_params)\n",
    "    \n",
    "    #print(\"params['batch_size']: \", params['batch_size'])\n",
    "    #print(\"params['max_lr']: \", params['max_lr'])\n",
    "    print(params)\n",
    "    early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=20, \n",
    "                               restore_best_weights = False)  # Define early stopping callback\n",
    "    \n",
    "    onecycle = OneCycleScheduler(math.ceil(len(X_train) / params['batch_size']) * n_epochs, max_rate=params['max_lr'])\n",
    "    \n",
    "    model_file = \"best_weights.h5\" # File to save the best weights\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(model_file, monitor=\"val_loss\", mode=\"min\", \n",
    "                                                    save_best_only=True, verbose=1) # Define checkpoint to save best weights\n",
    "    \n",
    "    history = model.fit(X_train_prep, y_train_pre_transf, callbacks=[early_stop, checkpoint, onecycle], \n",
    "                        validation_data=(X_val_prep, y_val), epochs=n_epochs, batch_size=params['batch_size'], verbose=0)\n",
    "    \n",
    "    model.load_weights(model_file) # reload the best weights saved by checkpoint\n",
    "    \n",
    "    loss = history.history['loss'][-1]\n",
    "    val_loss = history.history['val_loss'][-1]\n",
    "    \n",
    "    return {'loss': val_loss, 'train_loss': loss, 'status': STATUS_OK, 'model': model, 'history': history} \n",
    "    # switch places of the loss varaible so that hyperopt select the best yperparameter based on val_loss instead of loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 48, 'dropout_rate': 0.23470318126108242, 'max_lr': 0.04897420196729322, 'n_hidden': 9, 'n_neurons': 64, 'rho': 0.9811514099125452}\n",
      "                                                                           \n",
      "Epoch 1: val_loss improved from inf to 0.91336, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 2: val_loss improved from 0.91336 to 0.69347, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 3: val_loss did not improve from 0.69347\n",
      "\n",
      "                                                                           \n",
      "Epoch 4: val_loss did not improve from 0.69347\n",
      "\n",
      "                                                                           \n",
      "Epoch 5: val_loss did not improve from 0.69347\n",
      "\n",
      "                                                                           \n",
      "Epoch 6: val_loss did not improve from 0.69347\n",
      "\n",
      "                                                                           \n",
      "Epoch 7: val_loss improved from 0.69347 to 0.52983, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 8: val_loss improved from 0.52983 to 0.47856, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 9: val_loss did not improve from 0.47856\n",
      "\n",
      "                                                                           \n",
      "Epoch 10: val_loss did not improve from 0.47856\n",
      "\n",
      "                                                                           \n",
      "Epoch 11: val_loss did not improve from 0.47856\n",
      "\n",
      "                                                                           \n",
      "Epoch 12: val_loss did not improve from 0.47856\n",
      "\n",
      "                                                                           \n",
      "Epoch 13: val_loss did not improve from 0.47856\n",
      "\n",
      "                                                                           \n",
      "Epoch 14: val_loss did not improve from 0.47856\n",
      "\n",
      "                                                                           \n",
      "Epoch 15: val_loss did not improve from 0.47856\n",
      "\n",
      "                                                                           \n",
      "Epoch 16: val_loss did not improve from 0.47856\n",
      "\n",
      "                                                                           \n",
      "Epoch 17: val_loss did not improve from 0.47856\n",
      "\n",
      "                                                                           \n",
      "Epoch 18: val_loss did not improve from 0.47856\n",
      "\n",
      "                                                                           \n",
      "Epoch 19: val_loss did not improve from 0.47856\n",
      "\n",
      "                                                                           \n",
      "Epoch 20: val_loss did not improve from 0.47856\n",
      "\n",
      "                                                                           \n",
      "Epoch 21: val_loss did not improve from 0.47856\n",
      "\n",
      "                                                                           \n",
      "Epoch 22: val_loss did not improve from 0.47856\n",
      "\n",
      "                                                                           \n",
      "Epoch 23: val_loss did not improve from 0.47856\n",
      "\n",
      "                                                                           \n",
      "Epoch 24: val_loss did not improve from 0.47856\n",
      "\n",
      "                                                                           \n",
      "Epoch 25: val_loss did not improve from 0.47856\n",
      "\n",
      "                                                                           \n",
      "Epoch 26: val_loss did not improve from 0.47856\n",
      "\n",
      "                                                                           \n",
      "Epoch 27: val_loss did not improve from 0.47856\n",
      "\n",
      "                                                                           \n",
      "Epoch 28: val_loss did not improve from 0.47856\n",
      "\n",
      "{'batch_size': 48, 'dropout_rate': 0.29371815397452244, 'max_lr': 0.042002021029741765, 'n_hidden': 14, 'n_neurons': 32, 'rho': 0.8997449688438438}\n",
      "                                                                           \n",
      "Epoch 1: val_loss improved from inf to 1.09377, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 2: val_loss improved from 1.09377 to 0.99123, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 3: val_loss did not improve from 0.99123\n",
      "\n",
      "                                                                           \n",
      "Epoch 4: val_loss improved from 0.99123 to 0.93860, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 5: val_loss improved from 0.93860 to 0.84099, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 6: val_loss improved from 0.84099 to 0.82647, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 7: val_loss did not improve from 0.82647\n",
      "\n",
      "                                                                           \n",
      "Epoch 8: val_loss did not improve from 0.82647\n",
      "\n",
      "                                                                           \n",
      "Epoch 9: val_loss did not improve from 0.82647\n",
      "\n",
      "                                                                           \n",
      "Epoch 10: val_loss did not improve from 0.82647\n",
      "\n",
      "                                                                           \n",
      "Epoch 11: val_loss did not improve from 0.82647\n",
      "\n",
      "                                                                           \n",
      "Epoch 12: val_loss did not improve from 0.82647\n",
      "\n",
      "                                                                           \n",
      "Epoch 13: val_loss did not improve from 0.82647\n",
      "\n",
      "                                                                           \n",
      "Epoch 14: val_loss did not improve from 0.82647\n",
      "\n",
      "                                                                           \n",
      "Epoch 15: val_loss did not improve from 0.82647\n",
      "\n",
      "                                                                           \n",
      "Epoch 16: val_loss did not improve from 0.82647\n",
      "\n",
      "                                                                           \n",
      "Epoch 17: val_loss did not improve from 0.82647\n",
      "\n",
      "                                                                           \n",
      "Epoch 18: val_loss did not improve from 0.82647\n",
      "\n",
      "                                                                           \n",
      "Epoch 19: val_loss did not improve from 0.82647\n",
      "\n",
      "                                                                           \n",
      "Epoch 20: val_loss did not improve from 0.82647\n",
      "\n",
      "                                                                           \n",
      "Epoch 21: val_loss improved from 0.82647 to 0.78496, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 22: val_loss improved from 0.78496 to 0.72184, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 23: val_loss improved from 0.72184 to 0.67668, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 24: val_loss improved from 0.67668 to 0.66566, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 25: val_loss improved from 0.66566 to 0.64955, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 26: val_loss did not improve from 0.64955\n",
      "\n",
      "                                                                           \n",
      "Epoch 27: val_loss did not improve from 0.64955\n",
      "\n",
      "                                                                           \n",
      "Epoch 28: val_loss did not improve from 0.64955\n",
      "\n",
      "                                                                           \n",
      "Epoch 29: val_loss did not improve from 0.64955\n",
      "\n",
      "                                                                           \n",
      "Epoch 30: val_loss did not improve from 0.64955\n",
      "\n",
      "                                                                           \n",
      "Epoch 31: val_loss did not improve from 0.64955\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \n",
      "Epoch 32: val_loss did not improve from 0.64955\n",
      "\n",
      "                                                                           \n",
      "Epoch 33: val_loss did not improve from 0.64955\n",
      "\n",
      "                                                                           \n",
      "Epoch 34: val_loss did not improve from 0.64955\n",
      "\n",
      "                                                                           \n",
      "Epoch 35: val_loss did not improve from 0.64955\n",
      "\n",
      "                                                                           \n",
      "Epoch 36: val_loss did not improve from 0.64955\n",
      "\n",
      "                                                                           \n",
      "Epoch 37: val_loss did not improve from 0.64955\n",
      "\n",
      "                                                                           \n",
      "Epoch 38: val_loss did not improve from 0.64955\n",
      "\n",
      "                                                                           \n",
      "Epoch 39: val_loss did not improve from 0.64955\n",
      "\n",
      "                                                                           \n",
      "Epoch 40: val_loss did not improve from 0.64955\n",
      "\n",
      "                                                                           \n",
      "Epoch 41: val_loss improved from 0.64955 to 0.62143, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 42: val_loss did not improve from 0.62143\n",
      "\n",
      "                                                                           \n",
      "Epoch 43: val_loss did not improve from 0.62143\n",
      "\n",
      "                                                                           \n",
      "Epoch 44: val_loss did not improve from 0.62143\n",
      "\n",
      "                                                                           \n",
      "Epoch 45: val_loss did not improve from 0.62143\n",
      "\n",
      "                                                                           \n",
      "Epoch 46: val_loss did not improve from 0.62143\n",
      "\n",
      "                                                                           \n",
      "Epoch 47: val_loss did not improve from 0.62143\n",
      "\n",
      "                                                                           \n",
      "Epoch 48: val_loss did not improve from 0.62143\n",
      "\n",
      "                                                                           \n",
      "Epoch 49: val_loss did not improve from 0.62143\n",
      "\n",
      "                                                                           \n",
      "Epoch 50: val_loss did not improve from 0.62143\n",
      "\n",
      "                                                                           \n",
      "Epoch 51: val_loss did not improve from 0.62143\n",
      "\n",
      "                                                                           \n",
      "Epoch 52: val_loss did not improve from 0.62143\n",
      "\n",
      "                                                                           \n",
      "Epoch 53: val_loss did not improve from 0.62143\n",
      "\n",
      "                                                                           \n",
      "Epoch 54: val_loss did not improve from 0.62143\n",
      "\n",
      "                                                                           \n",
      "Epoch 55: val_loss improved from 0.62143 to 0.49408, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 56: val_loss improved from 0.49408 to 0.49199, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 57: val_loss did not improve from 0.49199\n",
      "\n",
      "                                                                           \n",
      "Epoch 58: val_loss did not improve from 0.49199\n",
      "\n",
      "                                                                           \n",
      "Epoch 59: val_loss did not improve from 0.49199\n",
      "\n",
      "                                                                           \n",
      "Epoch 60: val_loss did not improve from 0.49199\n",
      "\n",
      "                                                                           \n",
      "Epoch 61: val_loss did not improve from 0.49199\n",
      "\n",
      "                                                                           \n",
      "Epoch 62: val_loss did not improve from 0.49199\n",
      "\n",
      "                                                                           \n",
      "Epoch 63: val_loss did not improve from 0.49199\n",
      "\n",
      "                                                                           \n",
      "Epoch 64: val_loss did not improve from 0.49199\n",
      "\n",
      "                                                                           \n",
      "Epoch 65: val_loss did not improve from 0.49199\n",
      "\n",
      "                                                                           \n",
      "Epoch 66: val_loss did not improve from 0.49199\n",
      "\n",
      "                                                                           \n",
      "Epoch 67: val_loss did not improve from 0.49199\n",
      "\n",
      "                                                                           \n",
      "Epoch 68: val_loss did not improve from 0.49199\n",
      "\n",
      "                                                                           \n",
      "Epoch 69: val_loss did not improve from 0.49199\n",
      "\n",
      "                                                                           \n",
      "Epoch 70: val_loss did not improve from 0.49199\n",
      "\n",
      "                                                                           \n",
      "Epoch 71: val_loss did not improve from 0.49199\n",
      "\n",
      "                                                                           \n",
      "Epoch 72: val_loss did not improve from 0.49199\n",
      "\n",
      "                                                                           \n",
      "Epoch 73: val_loss did not improve from 0.49199\n",
      "\n",
      "                                                                           \n",
      "Epoch 74: val_loss did not improve from 0.49199\n",
      "\n",
      "                                                                           \n",
      "Epoch 75: val_loss did not improve from 0.49199\n",
      "\n",
      "                                                                           \n",
      "Epoch 76: val_loss did not improve from 0.49199\n",
      "\n",
      "{'batch_size': 48, 'dropout_rate': 0.16584088486195542, 'max_lr': 0.049507565830746066, 'n_hidden': 4, 'n_neurons': 32, 'rho': 0.8806586312185373}\n",
      "                                                                           \n",
      "Epoch 1: val_loss improved from inf to 0.66874, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 2: val_loss did not improve from 0.66874\n",
      "\n",
      "                                                                           \n",
      "Epoch 3: val_loss did not improve from 0.66874\n",
      "\n",
      "                                                                           \n",
      "Epoch 4: val_loss did not improve from 0.66874\n",
      "\n",
      "                                                                           \n",
      "Epoch 5: val_loss did not improve from 0.66874\n",
      "\n",
      "                                                                           \n",
      "Epoch 6: val_loss did not improve from 0.66874\n",
      "\n",
      "                                                                           \n",
      "Epoch 7: val_loss did not improve from 0.66874\n",
      "\n",
      "                                                                           \n",
      "Epoch 8: val_loss did not improve from 0.66874\n",
      "\n",
      "                                                                           \n",
      "Epoch 9: val_loss did not improve from 0.66874\n",
      "\n",
      "                                                                           \n",
      "Epoch 10: val_loss did not improve from 0.66874\n",
      "\n",
      "                                                                           \n",
      "Epoch 11: val_loss did not improve from 0.66874\n",
      "\n",
      "                                                                           \n",
      "Epoch 12: val_loss did not improve from 0.66874\n",
      "\n",
      "                                                                           \n",
      "Epoch 13: val_loss did not improve from 0.66874\n",
      "\n",
      "                                                                           \n",
      "Epoch 14: val_loss did not improve from 0.66874\n",
      "\n",
      "                                                                           \n",
      "Epoch 15: val_loss did not improve from 0.66874\n",
      "\n",
      "                                                                           \n",
      "Epoch 16: val_loss did not improve from 0.66874\n",
      "\n",
      "                                                                           \n",
      "Epoch 17: val_loss did not improve from 0.66874\n",
      "\n",
      "                                                                           \n",
      "Epoch 18: val_loss did not improve from 0.66874\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \n",
      "Epoch 19: val_loss did not improve from 0.66874\n",
      "\n",
      "                                                                           \n",
      "Epoch 20: val_loss did not improve from 0.66874\n",
      "\n",
      "                                                                           \n",
      "Epoch 21: val_loss did not improve from 0.66874\n",
      "\n",
      "{'batch_size': 32, 'dropout_rate': 0.1036328752059449, 'max_lr': 0.031775414051469456, 'n_hidden': 4, 'n_neurons': 64, 'rho': 0.8975392851785546}\n",
      "                                                                           \n",
      "Epoch 1: val_loss improved from inf to 1.48971, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 2: val_loss did not improve from 1.48971\n",
      "\n",
      "                                                                           \n",
      "Epoch 3: val_loss did not improve from 1.48971\n",
      "\n",
      "                                                                           \n",
      "Epoch 4: val_loss did not improve from 1.48971\n",
      "\n",
      "                                                                           \n",
      "Epoch 5: val_loss did not improve from 1.48971\n",
      "\n",
      "                                                                           \n",
      "Epoch 6: val_loss improved from 1.48971 to 1.29238, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 7: val_loss improved from 1.29238 to 1.21828, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 8: val_loss improved from 1.21828 to 0.99778, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 9: val_loss improved from 0.99778 to 0.96656, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 10: val_loss did not improve from 0.96656\n",
      "\n",
      "                                                                           \n",
      "Epoch 11: val_loss did not improve from 0.96656\n",
      "\n",
      "                                                                           \n",
      "Epoch 12: val_loss did not improve from 0.96656\n",
      "\n",
      "                                                                           \n",
      "Epoch 13: val_loss did not improve from 0.96656\n",
      "\n",
      "                                                                           \n",
      "Epoch 14: val_loss did not improve from 0.96656\n",
      "\n",
      "                                                                           \n",
      "Epoch 15: val_loss improved from 0.96656 to 0.92716, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 16: val_loss improved from 0.92716 to 0.91108, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 17: val_loss did not improve from 0.91108\n",
      "\n",
      "                                                                           \n",
      "Epoch 18: val_loss improved from 0.91108 to 0.82946, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 19: val_loss did not improve from 0.82946\n",
      "\n",
      "                                                                           \n",
      "Epoch 20: val_loss did not improve from 0.82946\n",
      "\n",
      "                                                                           \n",
      "Epoch 21: val_loss did not improve from 0.82946\n",
      "\n",
      "                                                                           \n",
      "Epoch 22: val_loss did not improve from 0.82946\n",
      "\n",
      "                                                                           \n",
      "Epoch 23: val_loss improved from 0.82946 to 0.81390, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 24: val_loss did not improve from 0.81390\n",
      "\n",
      "                                                                           \n",
      "Epoch 25: val_loss did not improve from 0.81390\n",
      "\n",
      "                                                                           \n",
      "Epoch 26: val_loss improved from 0.81390 to 0.81232, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 27: val_loss did not improve from 0.81232\n",
      "\n",
      "                                                                           \n",
      "Epoch 28: val_loss did not improve from 0.81232\n",
      "\n",
      "                                                                           \n",
      "Epoch 29: val_loss did not improve from 0.81232\n",
      "\n",
      "                                                                           \n",
      "Epoch 30: val_loss did not improve from 0.81232\n",
      "\n",
      "                                                                           \n",
      "Epoch 31: val_loss did not improve from 0.81232\n",
      "\n",
      "                                                                           \n",
      "Epoch 32: val_loss did not improve from 0.81232\n",
      "\n",
      "                                                                           \n",
      "Epoch 33: val_loss did not improve from 0.81232\n",
      "\n",
      "                                                                           \n",
      "Epoch 34: val_loss did not improve from 0.81232\n",
      "\n",
      "                                                                           \n",
      "Epoch 35: val_loss did not improve from 0.81232\n",
      "\n",
      "                                                                           \n",
      "Epoch 36: val_loss did not improve from 0.81232\n",
      "\n",
      "                                                                           \n",
      "Epoch 37: val_loss did not improve from 0.81232\n",
      "\n",
      "                                                                           \n",
      "Epoch 38: val_loss did not improve from 0.81232\n",
      "\n",
      "                                                                           \n",
      "Epoch 39: val_loss did not improve from 0.81232\n",
      "\n",
      "                                                                           \n",
      "Epoch 40: val_loss did not improve from 0.81232\n",
      "\n",
      "                                                                           \n",
      "Epoch 41: val_loss did not improve from 0.81232\n",
      "\n",
      "                                                                           \n",
      "Epoch 42: val_loss did not improve from 0.81232\n",
      "\n",
      "                                                                           \n",
      "Epoch 43: val_loss did not improve from 0.81232\n",
      "\n",
      "                                                                           \n",
      "Epoch 44: val_loss did not improve from 0.81232\n",
      "\n",
      "                                                                           \n",
      "Epoch 45: val_loss did not improve from 0.81232\n",
      "\n",
      "                                                                           \n",
      "Epoch 46: val_loss did not improve from 0.81232\n",
      "\n",
      "{'batch_size': 32, 'dropout_rate': 0.32450371049540583, 'max_lr': 0.04610021324439928, 'n_hidden': 14, 'n_neurons': 32, 'rho': 0.8368422194345613}\n",
      "                                                                           \n",
      "Epoch 1: val_loss improved from inf to 0.75108, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 2: val_loss did not improve from 0.75108\n",
      "\n",
      "                                                                           \n",
      "Epoch 3: val_loss did not improve from 0.75108\n",
      "\n",
      "                                                                           \n",
      "Epoch 4: val_loss did not improve from 0.75108\n",
      "\n",
      "                                                                           \n",
      "Epoch 5: val_loss did not improve from 0.75108\n",
      "\n",
      "                                                                           \n",
      "Epoch 6: val_loss did not improve from 0.75108\n",
      "\n",
      "                                                                           \n",
      "Epoch 7: val_loss did not improve from 0.75108\n",
      "\n",
      "                                                                           \n",
      "Epoch 8: val_loss did not improve from 0.75108\n",
      "\n",
      "                                                                           \n",
      "Epoch 9: val_loss did not improve from 0.75108\n",
      "\n",
      "                                                                           \n",
      "Epoch 10: val_loss did not improve from 0.75108\n",
      "\n",
      "                                                                           \n",
      "Epoch 11: val_loss did not improve from 0.75108\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \n",
      "Epoch 12: val_loss did not improve from 0.75108\n",
      "\n",
      "                                                                           \n",
      "Epoch 13: val_loss did not improve from 0.75108\n",
      "\n",
      "                                                                           \n",
      "Epoch 14: val_loss did not improve from 0.75108\n",
      "\n",
      "                                                                           \n",
      "Epoch 15: val_loss did not improve from 0.75108\n",
      "\n",
      "                                                                           \n",
      "Epoch 16: val_loss did not improve from 0.75108\n",
      "\n",
      "                                                                           \n",
      "Epoch 17: val_loss did not improve from 0.75108\n",
      "\n",
      "                                                                           \n",
      "Epoch 18: val_loss did not improve from 0.75108\n",
      "\n",
      "                                                                           \n",
      "Epoch 19: val_loss did not improve from 0.75108\n",
      "\n",
      "                                                                           \n",
      "Epoch 20: val_loss did not improve from 0.75108\n",
      "\n",
      "                                                                           \n",
      "Epoch 21: val_loss did not improve from 0.75108\n",
      "\n",
      "{'batch_size': 48, 'dropout_rate': 0.2565993454139373, 'max_lr': 0.04114039842047148, 'n_hidden': 14, 'n_neurons': 128, 'rho': 0.9833791121946147}\n",
      "                                                                           \n",
      "Epoch 1: val_loss improved from inf to 0.69677, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 2: val_loss did not improve from 0.69677\n",
      "\n",
      "                                                                           \n",
      "Epoch 3: val_loss did not improve from 0.69677\n",
      "\n",
      "                                                                           \n",
      "Epoch 4: val_loss did not improve from 0.69677\n",
      "\n",
      "                                                                           \n",
      "Epoch 5: val_loss improved from 0.69677 to 0.45820, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 6: val_loss did not improve from 0.45820\n",
      "\n",
      "                                                                           \n",
      "Epoch 7: val_loss did not improve from 0.45820\n",
      "\n",
      "                                                                           \n",
      "Epoch 8: val_loss did not improve from 0.45820\n",
      "\n",
      "                                                                           \n",
      "Epoch 9: val_loss did not improve from 0.45820\n",
      "\n",
      "                                                                           \n",
      "Epoch 10: val_loss did not improve from 0.45820\n",
      "\n",
      "                                                                           \n",
      "Epoch 11: val_loss did not improve from 0.45820\n",
      "\n",
      "                                                                           \n",
      "Epoch 12: val_loss did not improve from 0.45820\n",
      "\n",
      "                                                                           \n",
      "Epoch 13: val_loss did not improve from 0.45820\n",
      "\n",
      "                                                                           \n",
      "Epoch 14: val_loss did not improve from 0.45820\n",
      "\n",
      "                                                                           \n",
      "Epoch 15: val_loss did not improve from 0.45820\n",
      "\n",
      "                                                                           \n",
      "Epoch 16: val_loss did not improve from 0.45820\n",
      "\n",
      "                                                                           \n",
      "Epoch 17: val_loss did not improve from 0.45820\n",
      "\n",
      "                                                                           \n",
      "Epoch 18: val_loss did not improve from 0.45820\n",
      "\n",
      "                                                                           \n",
      "Epoch 19: val_loss did not improve from 0.45820\n",
      "\n",
      "                                                                           \n",
      "Epoch 20: val_loss did not improve from 0.45820\n",
      "\n",
      "                                                                           \n",
      "Epoch 21: val_loss did not improve from 0.45820\n",
      "\n",
      "                                                                           \n",
      "Epoch 22: val_loss did not improve from 0.45820\n",
      "\n",
      "                                                                           \n",
      "Epoch 23: val_loss did not improve from 0.45820\n",
      "\n",
      "                                                                           \n",
      "Epoch 24: val_loss did not improve from 0.45820\n",
      "\n",
      "                                                                           \n",
      "Epoch 25: val_loss did not improve from 0.45820\n",
      "\n",
      "{'batch_size': 48, 'dropout_rate': 0.36227541106991734, 'max_lr': 0.04526110671172121, 'n_hidden': 9, 'n_neurons': 32, 'rho': 0.7459603686329034}\n",
      "                                                                           \n",
      "Epoch 1: val_loss improved from inf to 1.18368, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 2: val_loss improved from 1.18368 to 1.09140, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 3: val_loss did not improve from 1.09140\n",
      "\n",
      "                                                                           \n",
      "Epoch 4: val_loss did not improve from 1.09140\n",
      "\n",
      "                                                                           \n",
      "Epoch 5: val_loss did not improve from 1.09140\n",
      "\n",
      "                                                                           \n",
      "Epoch 6: val_loss did not improve from 1.09140\n",
      "\n",
      "                                                                           \n",
      "Epoch 7: val_loss did not improve from 1.09140\n",
      "\n",
      "                                                                           \n",
      "Epoch 8: val_loss improved from 1.09140 to 0.94631, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 9: val_loss improved from 0.94631 to 0.85762, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 10: val_loss improved from 0.85762 to 0.83667, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 11: val_loss improved from 0.83667 to 0.81787, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 12: val_loss did not improve from 0.81787\n",
      "\n",
      "                                                                           \n",
      "Epoch 13: val_loss did not improve from 0.81787\n",
      "\n",
      "                                                                           \n",
      "Epoch 14: val_loss did not improve from 0.81787\n",
      "\n",
      "                                                                           \n",
      "Epoch 15: val_loss did not improve from 0.81787\n",
      "\n",
      "                                                                           \n",
      "Epoch 16: val_loss did not improve from 0.81787\n",
      "\n",
      "                                                                           \n",
      "Epoch 17: val_loss did not improve from 0.81787\n",
      "\n",
      "                                                                           \n",
      "Epoch 18: val_loss did not improve from 0.81787\n",
      "\n",
      "                                                                           \n",
      "Epoch 19: val_loss did not improve from 0.81787\n",
      "\n",
      "                                                                           \n",
      "Epoch 20: val_loss did not improve from 0.81787\n",
      "\n",
      "                                                                           \n",
      "Epoch 21: val_loss did not improve from 0.81787\n",
      "\n",
      "                                                                           \n",
      "Epoch 22: val_loss did not improve from 0.81787\n",
      "\n",
      "                                                                           \n",
      "Epoch 23: val_loss did not improve from 0.81787\n",
      "\n",
      "                                                                           \n",
      "Epoch 24: val_loss did not improve from 0.81787\n",
      "\n",
      "                                                                           \n",
      "Epoch 25: val_loss did not improve from 0.81787\n",
      "\n",
      "                                                                           \n",
      "Epoch 26: val_loss did not improve from 0.81787\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \n",
      "Epoch 27: val_loss did not improve from 0.81787\n",
      "\n",
      "                                                                           \n",
      "Epoch 28: val_loss did not improve from 0.81787\n",
      "\n",
      "                                                                           \n",
      "Epoch 29: val_loss did not improve from 0.81787\n",
      "\n",
      "                                                                           \n",
      "Epoch 30: val_loss did not improve from 0.81787\n",
      "\n",
      "                                                                           \n",
      "Epoch 31: val_loss did not improve from 0.81787\n",
      "\n",
      "{'batch_size': 64, 'dropout_rate': 0.28181808624951654, 'max_lr': 0.04537608465903134, 'n_hidden': 9, 'n_neurons': 32, 'rho': 0.8740009000916202}\n",
      "                                                                           \n",
      "Epoch 1: val_loss improved from inf to 1.23811, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 2: val_loss improved from 1.23811 to 0.83500, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 3: val_loss improved from 0.83500 to 0.73783, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 4: val_loss improved from 0.73783 to 0.71348, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 5: val_loss improved from 0.71348 to 0.66281, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 6: val_loss improved from 0.66281 to 0.59425, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 7: val_loss improved from 0.59425 to 0.53132, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 8: val_loss improved from 0.53132 to 0.48810, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 9: val_loss improved from 0.48810 to 0.47051, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 10: val_loss improved from 0.47051 to 0.46431, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 11: val_loss improved from 0.46431 to 0.44895, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 12: val_loss improved from 0.44895 to 0.43997, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 13: val_loss did not improve from 0.43997\n",
      "\n",
      "                                                                           \n",
      "Epoch 14: val_loss improved from 0.43997 to 0.43243, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 15: val_loss did not improve from 0.43243\n",
      "\n",
      "                                                                           \n",
      "Epoch 16: val_loss did not improve from 0.43243\n",
      "\n",
      "                                                                           \n",
      "Epoch 17: val_loss did not improve from 0.43243\n",
      "\n",
      "                                                                           \n",
      "Epoch 18: val_loss did not improve from 0.43243\n",
      "\n",
      "                                                                           \n",
      "Epoch 19: val_loss did not improve from 0.43243\n",
      "\n",
      "                                                                           \n",
      "Epoch 20: val_loss did not improve from 0.43243\n",
      "\n",
      "                                                                           \n",
      "Epoch 21: val_loss did not improve from 0.43243\n",
      "\n",
      "                                                                           \n",
      "Epoch 22: val_loss did not improve from 0.43243\n",
      "\n",
      "                                                                           \n",
      "Epoch 23: val_loss did not improve from 0.43243\n",
      "\n",
      "                                                                           \n",
      "Epoch 24: val_loss did not improve from 0.43243\n",
      "\n",
      "                                                                           \n",
      "Epoch 25: val_loss did not improve from 0.43243\n",
      "\n",
      "                                                                           \n",
      "Epoch 26: val_loss did not improve from 0.43243\n",
      "\n",
      "                                                                           \n",
      "Epoch 27: val_loss did not improve from 0.43243\n",
      "\n",
      "                                                                           \n",
      "Epoch 28: val_loss did not improve from 0.43243\n",
      "\n",
      "                                                                           \n",
      "Epoch 29: val_loss did not improve from 0.43243\n",
      "\n",
      "                                                                           \n",
      "Epoch 30: val_loss did not improve from 0.43243\n",
      "\n",
      "                                                                           \n",
      "Epoch 31: val_loss did not improve from 0.43243\n",
      "\n",
      "                                                                           \n",
      "Epoch 32: val_loss did not improve from 0.43243\n",
      "\n",
      "                                                                           \n",
      "Epoch 33: val_loss did not improve from 0.43243\n",
      "\n",
      "                                                                           \n",
      "Epoch 34: val_loss did not improve from 0.43243\n",
      "\n",
      "{'batch_size': 64, 'dropout_rate': 0.11117825575161391, 'max_lr': 0.04835783377614596, 'n_hidden': 9, 'n_neurons': 32, 'rho': 0.7482943819584433}\n",
      "                                                                           \n",
      "Epoch 1: val_loss improved from inf to 0.83942, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 2: val_loss did not improve from 0.83942\n",
      "\n",
      "                                                                           \n",
      "Epoch 3: val_loss did not improve from 0.83942\n",
      "\n",
      "                                                                           \n",
      "Epoch 4: val_loss did not improve from 0.83942\n",
      "\n",
      "                                                                           \n",
      "Epoch 5: val_loss did not improve from 0.83942\n",
      "\n",
      "                                                                           \n",
      "Epoch 6: val_loss did not improve from 0.83942\n",
      "\n",
      "                                                                           \n",
      "Epoch 7: val_loss did not improve from 0.83942\n",
      "\n",
      "                                                                           \n",
      "Epoch 8: val_loss did not improve from 0.83942\n",
      "\n",
      "                                                                           \n",
      "Epoch 9: val_loss did not improve from 0.83942\n",
      "\n",
      "                                                                           \n",
      "Epoch 10: val_loss did not improve from 0.83942\n",
      "\n",
      "                                                                           \n",
      "Epoch 11: val_loss did not improve from 0.83942\n",
      "\n",
      "                                                                           \n",
      "Epoch 12: val_loss did not improve from 0.83942\n",
      "\n",
      "                                                                           \n",
      "Epoch 13: val_loss did not improve from 0.83942\n",
      "\n",
      "                                                                           \n",
      "Epoch 14: val_loss did not improve from 0.83942\n",
      "\n",
      "                                                                           \n",
      "Epoch 15: val_loss did not improve from 0.83942\n",
      "\n",
      "                                                                           \n",
      "Epoch 16: val_loss did not improve from 0.83942\n",
      "\n",
      "                                                                           \n",
      "Epoch 17: val_loss did not improve from 0.83942\n",
      "\n",
      "                                                                           \n",
      "Epoch 18: val_loss did not improve from 0.83942\n",
      "\n",
      "                                                                           \n",
      "Epoch 19: val_loss did not improve from 0.83942\n",
      "\n",
      "                                                                           \n",
      "Epoch 20: val_loss did not improve from 0.83942\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \n",
      "Epoch 21: val_loss did not improve from 0.83942\n",
      "\n",
      "{'batch_size': 32, 'dropout_rate': 0.39518796792469324, 'max_lr': 0.034849055704337675, 'n_hidden': 14, 'n_neurons': 64, 'rho': 0.7054184334928892}\n",
      "                                                                           \n",
      "Epoch 1: val_loss improved from inf to 1.63864, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 2: val_loss did not improve from 1.63864\n",
      "\n",
      "                                                                           \n",
      "Epoch 3: val_loss did not improve from 1.63864\n",
      "\n",
      "                                                                           \n",
      "Epoch 4: val_loss did not improve from 1.63864\n",
      "\n",
      "                                                                           \n",
      "Epoch 5: val_loss did not improve from 1.63864\n",
      "\n",
      "                                                                           \n",
      "Epoch 6: val_loss did not improve from 1.63864\n",
      "\n",
      "                                                                           \n",
      "Epoch 7: val_loss improved from 1.63864 to 1.46145, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 8: val_loss improved from 1.46145 to 1.18248, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 9: val_loss improved from 1.18248 to 1.03439, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 10: val_loss improved from 1.03439 to 0.98833, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 11: val_loss improved from 0.98833 to 0.90041, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 12: val_loss improved from 0.90041 to 0.78408, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 13: val_loss improved from 0.78408 to 0.68111, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 14: val_loss improved from 0.68111 to 0.62419, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 15: val_loss did not improve from 0.62419\n",
      "\n",
      "                                                                           \n",
      "Epoch 16: val_loss did not improve from 0.62419\n",
      "\n",
      "                                                                           \n",
      "Epoch 17: val_loss improved from 0.62419 to 0.58635, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 18: val_loss improved from 0.58635 to 0.54435, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 19: val_loss improved from 0.54435 to 0.52365, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 20: val_loss improved from 0.52365 to 0.50896, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 21: val_loss did not improve from 0.50896\n",
      "\n",
      "                                                                           \n",
      "Epoch 22: val_loss did not improve from 0.50896\n",
      "\n",
      "                                                                           \n",
      "Epoch 23: val_loss did not improve from 0.50896\n",
      "\n",
      "                                                                           \n",
      "Epoch 24: val_loss did not improve from 0.50896\n",
      "\n",
      "                                                                           \n",
      "Epoch 25: val_loss did not improve from 0.50896\n",
      "\n",
      "                                                                           \n",
      "Epoch 26: val_loss did not improve from 0.50896\n",
      "\n",
      "                                                                           \n",
      "Epoch 27: val_loss did not improve from 0.50896\n",
      "\n",
      "                                                                           \n",
      "Epoch 28: val_loss did not improve from 0.50896\n",
      "\n",
      "                                                                           \n",
      "Epoch 29: val_loss did not improve from 0.50896\n",
      "\n",
      "                                                                           \n",
      "Epoch 30: val_loss did not improve from 0.50896\n",
      "\n",
      "                                                                           \n",
      "Epoch 31: val_loss did not improve from 0.50896\n",
      "\n",
      "                                                                           \n",
      "Epoch 32: val_loss did not improve from 0.50896\n",
      "\n",
      "                                                                           \n",
      "Epoch 33: val_loss did not improve from 0.50896\n",
      "\n",
      "                                                                           \n",
      "Epoch 34: val_loss did not improve from 0.50896\n",
      "\n",
      "                                                                           \n",
      "Epoch 35: val_loss did not improve from 0.50896\n",
      "\n",
      "                                                                           \n",
      "Epoch 36: val_loss did not improve from 0.50896\n",
      "\n",
      "                                                                           \n",
      "Epoch 37: val_loss did not improve from 0.50896\n",
      "\n",
      "                                                                           \n",
      "Epoch 38: val_loss did not improve from 0.50896\n",
      "\n",
      "                                                                           \n",
      "Epoch 39: val_loss did not improve from 0.50896\n",
      "\n",
      "                                                                           \n",
      "Epoch 40: val_loss did not improve from 0.50896\n",
      "\n",
      "{'batch_size': 64, 'dropout_rate': 0.1603391027600698, 'max_lr': 0.042262775101621686, 'n_hidden': 4, 'n_neurons': 64, 'rho': 0.8396090338170584}\n",
      "                                                                           \n",
      "Epoch 1: val_loss improved from inf to 0.93123, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 2: val_loss did not improve from 0.93123\n",
      "\n",
      "                                                                           \n",
      "Epoch 3: val_loss did not improve from 0.93123\n",
      "\n",
      "                                                                           \n",
      "Epoch 4: val_loss did not improve from 0.93123\n",
      "\n",
      "                                                                           \n",
      "Epoch 5: val_loss did not improve from 0.93123\n",
      "\n",
      "                                                                           \n",
      "Epoch 6: val_loss did not improve from 0.93123\n",
      "\n",
      "                                                                           \n",
      "Epoch 7: val_loss did not improve from 0.93123\n",
      "\n",
      "                                                                           \n",
      "Epoch 8: val_loss did not improve from 0.93123\n",
      "\n",
      "                                                                           \n",
      "Epoch 9: val_loss did not improve from 0.93123\n",
      "\n",
      "                                                                           \n",
      "Epoch 10: val_loss did not improve from 0.93123\n",
      "\n",
      "                                                                           \n",
      "Epoch 11: val_loss did not improve from 0.93123\n",
      "\n",
      "                                                                           \n",
      "Epoch 12: val_loss did not improve from 0.93123\n",
      "\n",
      "                                                                           \n",
      "Epoch 13: val_loss did not improve from 0.93123\n",
      "\n",
      "                                                                           \n",
      "Epoch 14: val_loss did not improve from 0.93123\n",
      "\n",
      "                                                                           \n",
      "Epoch 15: val_loss did not improve from 0.93123\n",
      "\n",
      "                                                                           \n",
      "Epoch 16: val_loss did not improve from 0.93123\n",
      "\n",
      "                                                                           \n",
      "Epoch 17: val_loss did not improve from 0.93123\n",
      "\n",
      "                                                                           \n",
      "Epoch 18: val_loss did not improve from 0.93123\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \n",
      "Epoch 19: val_loss did not improve from 0.93123\n",
      "\n",
      "                                                                           \n",
      "Epoch 20: val_loss did not improve from 0.93123\n",
      "\n",
      "                                                                           \n",
      "Epoch 21: val_loss did not improve from 0.93123\n",
      "\n",
      "{'batch_size': 48, 'dropout_rate': 0.24303104060874167, 'max_lr': 0.03036708346343618, 'n_hidden': 4, 'n_neurons': 32, 'rho': 0.7216978696298072}\n",
      "                                                                           \n",
      "Epoch 1: val_loss improved from inf to 0.66312, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 2: val_loss improved from 0.66312 to 0.53426, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 3: val_loss did not improve from 0.53426\n",
      "\n",
      "                                                                           \n",
      "Epoch 4: val_loss did not improve from 0.53426\n",
      "\n",
      "                                                                           \n",
      "Epoch 5: val_loss did not improve from 0.53426\n",
      "\n",
      "                                                                           \n",
      "Epoch 6: val_loss did not improve from 0.53426\n",
      "\n",
      "                                                                           \n",
      "Epoch 7: val_loss did not improve from 0.53426\n",
      "\n",
      "                                                                           \n",
      "Epoch 8: val_loss did not improve from 0.53426\n",
      "\n",
      "                                                                           \n",
      "Epoch 9: val_loss did not improve from 0.53426\n",
      "\n",
      "                                                                           \n",
      "Epoch 10: val_loss did not improve from 0.53426\n",
      "\n",
      "                                                                           \n",
      "Epoch 11: val_loss did not improve from 0.53426\n",
      "\n",
      "                                                                           \n",
      "Epoch 12: val_loss did not improve from 0.53426\n",
      "\n",
      "                                                                           \n",
      "Epoch 13: val_loss did not improve from 0.53426\n",
      "\n",
      "                                                                           \n",
      "Epoch 14: val_loss did not improve from 0.53426\n",
      "\n",
      "                                                                           \n",
      "Epoch 15: val_loss did not improve from 0.53426\n",
      "\n",
      "                                                                           \n",
      "Epoch 16: val_loss did not improve from 0.53426\n",
      "\n",
      "                                                                           \n",
      "Epoch 17: val_loss did not improve from 0.53426\n",
      "\n",
      "                                                                           \n",
      "Epoch 18: val_loss did not improve from 0.53426\n",
      "\n",
      "                                                                           \n",
      "Epoch 19: val_loss did not improve from 0.53426\n",
      "\n",
      "                                                                           \n",
      "Epoch 20: val_loss did not improve from 0.53426\n",
      "\n",
      "                                                                           \n",
      "Epoch 21: val_loss did not improve from 0.53426\n",
      "\n",
      "                                                                           \n",
      "Epoch 22: val_loss did not improve from 0.53426\n",
      "\n",
      "{'batch_size': 32, 'dropout_rate': 0.10808767008596301, 'max_lr': 0.04971956099758461, 'n_hidden': 9, 'n_neurons': 32, 'rho': 0.8849198942401352}\n",
      "                                                                           \n",
      "Epoch 1: val_loss improved from inf to 0.84538, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 2: val_loss improved from 0.84538 to 0.82389, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 3: val_loss improved from 0.82389 to 0.56277, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 4: val_loss improved from 0.56277 to 0.55681, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 5: val_loss did not improve from 0.55681\n",
      "\n",
      "                                                                           \n",
      "Epoch 6: val_loss did not improve from 0.55681\n",
      "\n",
      "                                                                           \n",
      "Epoch 7: val_loss did not improve from 0.55681\n",
      "\n",
      "                                                                           \n",
      "Epoch 8: val_loss did not improve from 0.55681\n",
      "\n",
      "                                                                           \n",
      "Epoch 9: val_loss did not improve from 0.55681\n",
      "\n",
      "                                                                           \n",
      "Epoch 10: val_loss did not improve from 0.55681\n",
      "\n",
      "                                                                           \n",
      "Epoch 11: val_loss did not improve from 0.55681\n",
      "\n",
      "                                                                           \n",
      "Epoch 12: val_loss did not improve from 0.55681\n",
      "\n",
      "                                                                           \n",
      "Epoch 13: val_loss did not improve from 0.55681\n",
      "\n",
      "                                                                           \n",
      "Epoch 14: val_loss did not improve from 0.55681\n",
      "\n",
      "                                                                           \n",
      "Epoch 15: val_loss did not improve from 0.55681\n",
      "\n",
      "                                                                           \n",
      "Epoch 16: val_loss did not improve from 0.55681\n",
      "\n",
      "                                                                           \n",
      "Epoch 17: val_loss did not improve from 0.55681\n",
      "\n",
      "                                                                           \n",
      "Epoch 18: val_loss did not improve from 0.55681\n",
      "\n",
      "                                                                           \n",
      "Epoch 19: val_loss did not improve from 0.55681\n",
      "\n",
      "                                                                           \n",
      "Epoch 20: val_loss did not improve from 0.55681\n",
      "\n",
      "                                                                           \n",
      "Epoch 21: val_loss did not improve from 0.55681\n",
      "\n",
      "                                                                           \n",
      "Epoch 22: val_loss did not improve from 0.55681\n",
      "\n",
      "                                                                           \n",
      "Epoch 23: val_loss did not improve from 0.55681\n",
      "\n",
      "                                                                           \n",
      "Epoch 24: val_loss did not improve from 0.55681\n",
      "\n",
      "{'batch_size': 64, 'dropout_rate': 0.2576611873603596, 'max_lr': 0.0319775826564818, 'n_hidden': 14, 'n_neurons': 128, 'rho': 0.9580914135843129}\n",
      "                                                                           \n",
      "Epoch 1: val_loss improved from inf to 0.79913, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 2: val_loss improved from 0.79913 to 0.65701, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 3: val_loss did not improve from 0.65701\n",
      "\n",
      "                                                                           \n",
      "Epoch 4: val_loss did not improve from 0.65701\n",
      "\n",
      "                                                                           \n",
      "Epoch 5: val_loss did not improve from 0.65701\n",
      "\n",
      "                                                                           \n",
      "Epoch 6: val_loss did not improve from 0.65701\n",
      "\n",
      "                                                                           \n",
      "Epoch 7: val_loss improved from 0.65701 to 0.58342, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 8: val_loss did not improve from 0.58342\n",
      "\n",
      "                                                                           \n",
      "Epoch 9: val_loss did not improve from 0.58342\n",
      "\n",
      "                                                                           \n",
      "Epoch 10: val_loss did not improve from 0.58342\n",
      "\n",
      "                                                                           \n",
      "Epoch 11: val_loss did not improve from 0.58342\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \n",
      "Epoch 12: val_loss did not improve from 0.58342\n",
      "\n",
      "                                                                           \n",
      "Epoch 13: val_loss did not improve from 0.58342\n",
      "\n",
      "                                                                           \n",
      "Epoch 14: val_loss did not improve from 0.58342\n",
      "\n",
      "                                                                           \n",
      "Epoch 15: val_loss did not improve from 0.58342\n",
      "\n",
      "                                                                           \n",
      "Epoch 16: val_loss did not improve from 0.58342\n",
      "\n",
      "                                                                           \n",
      "Epoch 17: val_loss did not improve from 0.58342\n",
      "\n",
      "                                                                           \n",
      "Epoch 18: val_loss did not improve from 0.58342\n",
      "\n",
      "                                                                           \n",
      "Epoch 19: val_loss improved from 0.58342 to 0.56091, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 20: val_loss did not improve from 0.56091\n",
      "\n",
      "                                                                           \n",
      "Epoch 21: val_loss did not improve from 0.56091\n",
      "\n",
      "                                                                           \n",
      "Epoch 22: val_loss did not improve from 0.56091\n",
      "\n",
      "                                                                           \n",
      "Epoch 23: val_loss improved from 0.56091 to 0.52070, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 24: val_loss improved from 0.52070 to 0.48746, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 25: val_loss did not improve from 0.48746\n",
      "\n",
      "                                                                           \n",
      "Epoch 26: val_loss did not improve from 0.48746\n",
      "\n",
      "                                                                           \n",
      "Epoch 27: val_loss did not improve from 0.48746\n",
      "\n",
      "                                                                           \n",
      "Epoch 28: val_loss did not improve from 0.48746\n",
      "\n",
      "                                                                           \n",
      "Epoch 29: val_loss did not improve from 0.48746\n",
      "\n",
      "                                                                           \n",
      "Epoch 30: val_loss did not improve from 0.48746\n",
      "\n",
      "                                                                           \n",
      "Epoch 31: val_loss did not improve from 0.48746\n",
      "\n",
      "                                                                           \n",
      "Epoch 32: val_loss did not improve from 0.48746\n",
      "\n",
      "                                                                           \n",
      "Epoch 33: val_loss did not improve from 0.48746\n",
      "\n",
      "                                                                           \n",
      "Epoch 34: val_loss did not improve from 0.48746\n",
      "\n",
      "                                                                           \n",
      "Epoch 35: val_loss did not improve from 0.48746\n",
      "\n",
      "                                                                           \n",
      "Epoch 36: val_loss did not improve from 0.48746\n",
      "\n",
      "                                                                           \n",
      "Epoch 37: val_loss did not improve from 0.48746\n",
      "\n",
      "                                                                           \n",
      "Epoch 38: val_loss did not improve from 0.48746\n",
      "\n",
      "                                                                           \n",
      "Epoch 39: val_loss did not improve from 0.48746\n",
      "\n",
      "                                                                           \n",
      "Epoch 40: val_loss did not improve from 0.48746\n",
      "\n",
      "                                                                           \n",
      "Epoch 41: val_loss did not improve from 0.48746\n",
      "\n",
      "                                                                           \n",
      "Epoch 42: val_loss did not improve from 0.48746\n",
      "\n",
      "                                                                           \n",
      "Epoch 43: val_loss did not improve from 0.48746\n",
      "\n",
      "                                                                           \n",
      "Epoch 44: val_loss did not improve from 0.48746\n",
      "\n",
      "{'batch_size': 48, 'dropout_rate': 0.289626442919109, 'max_lr': 0.03853292182085625, 'n_hidden': 9, 'n_neurons': 64, 'rho': 0.8046023396127595}\n",
      "                                                                           \n",
      "Epoch 1: val_loss improved from inf to 1.60948, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 2: val_loss improved from 1.60948 to 1.53827, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 3: val_loss improved from 1.53827 to 1.23678, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 4: val_loss improved from 1.23678 to 0.99682, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 5: val_loss improved from 0.99682 to 0.90649, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 6: val_loss did not improve from 0.90649\n",
      "\n",
      "                                                                           \n",
      "Epoch 7: val_loss did not improve from 0.90649\n",
      "\n",
      "                                                                           \n",
      "Epoch 8: val_loss did not improve from 0.90649\n",
      "\n",
      "                                                                           \n",
      "Epoch 9: val_loss did not improve from 0.90649\n",
      "\n",
      "                                                                           \n",
      "Epoch 10: val_loss did not improve from 0.90649\n",
      "\n",
      "                                                                           \n",
      "Epoch 11: val_loss did not improve from 0.90649\n",
      "\n",
      "                                                                           \n",
      "Epoch 12: val_loss did not improve from 0.90649\n",
      "\n",
      "                                                                           \n",
      "Epoch 13: val_loss did not improve from 0.90649\n",
      "\n",
      "                                                                           \n",
      "Epoch 14: val_loss did not improve from 0.90649\n",
      "\n",
      "                                                                           \n",
      "Epoch 15: val_loss did not improve from 0.90649\n",
      "\n",
      "                                                                           \n",
      "Epoch 16: val_loss did not improve from 0.90649\n",
      "\n",
      "                                                                           \n",
      "Epoch 17: val_loss did not improve from 0.90649\n",
      "\n",
      "                                                                           \n",
      "Epoch 18: val_loss did not improve from 0.90649\n",
      "\n",
      "                                                                           \n",
      "Epoch 19: val_loss did not improve from 0.90649\n",
      "\n",
      "                                                                           \n",
      "Epoch 20: val_loss did not improve from 0.90649\n",
      "\n",
      "                                                                           \n",
      "Epoch 21: val_loss did not improve from 0.90649\n",
      "\n",
      "                                                                           \n",
      "Epoch 22: val_loss did not improve from 0.90649\n",
      "\n",
      "                                                                           \n",
      "Epoch 23: val_loss improved from 0.90649 to 0.83128, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 24: val_loss improved from 0.83128 to 0.51788, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 25: val_loss did not improve from 0.51788\n",
      "\n",
      "                                                                           \n",
      "Epoch 26: val_loss did not improve from 0.51788\n",
      "\n",
      "                                                                           \n",
      "Epoch 27: val_loss did not improve from 0.51788\n",
      "\n",
      "                                                                           \n",
      "Epoch 28: val_loss did not improve from 0.51788\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \n",
      "Epoch 29: val_loss did not improve from 0.51788\n",
      "\n",
      "                                                                           \n",
      "Epoch 30: val_loss did not improve from 0.51788\n",
      "\n",
      "                                                                           \n",
      "Epoch 31: val_loss did not improve from 0.51788\n",
      "\n",
      "                                                                           \n",
      "Epoch 32: val_loss did not improve from 0.51788\n",
      "\n",
      "                                                                           \n",
      "Epoch 33: val_loss did not improve from 0.51788\n",
      "\n",
      "                                                                           \n",
      "Epoch 34: val_loss did not improve from 0.51788\n",
      "\n",
      "                                                                           \n",
      "Epoch 35: val_loss did not improve from 0.51788\n",
      "\n",
      "                                                                           \n",
      "Epoch 36: val_loss did not improve from 0.51788\n",
      "\n",
      "                                                                           \n",
      "Epoch 37: val_loss did not improve from 0.51788\n",
      "\n",
      "                                                                           \n",
      "Epoch 38: val_loss did not improve from 0.51788\n",
      "\n",
      "                                                                           \n",
      "Epoch 39: val_loss improved from 0.51788 to 0.51205, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 40: val_loss improved from 0.51205 to 0.46980, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 41: val_loss improved from 0.46980 to 0.43389, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 42: val_loss did not improve from 0.43389\n",
      "\n",
      "                                                                           \n",
      "Epoch 43: val_loss did not improve from 0.43389\n",
      "\n",
      "                                                                           \n",
      "Epoch 44: val_loss did not improve from 0.43389\n",
      "\n",
      "                                                                           \n",
      "Epoch 45: val_loss did not improve from 0.43389\n",
      "\n",
      "                                                                           \n",
      "Epoch 46: val_loss did not improve from 0.43389\n",
      "\n",
      "                                                                           \n",
      "Epoch 47: val_loss did not improve from 0.43389\n",
      "\n",
      "                                                                           \n",
      "Epoch 48: val_loss did not improve from 0.43389\n",
      "\n",
      "                                                                           \n",
      "Epoch 49: val_loss did not improve from 0.43389\n",
      "\n",
      "                                                                           \n",
      "Epoch 50: val_loss did not improve from 0.43389\n",
      "\n",
      "                                                                           \n",
      "Epoch 51: val_loss did not improve from 0.43389\n",
      "\n",
      "                                                                           \n",
      "Epoch 52: val_loss did not improve from 0.43389\n",
      "\n",
      "                                                                           \n",
      "Epoch 53: val_loss did not improve from 0.43389\n",
      "\n",
      "                                                                           \n",
      "Epoch 54: val_loss did not improve from 0.43389\n",
      "\n",
      "                                                                           \n",
      "Epoch 55: val_loss did not improve from 0.43389\n",
      "\n",
      "                                                                           \n",
      "Epoch 56: val_loss did not improve from 0.43389\n",
      "\n",
      "                                                                           \n",
      "Epoch 57: val_loss did not improve from 0.43389\n",
      "\n",
      "                                                                           \n",
      "Epoch 58: val_loss did not improve from 0.43389\n",
      "\n",
      "                                                                           \n",
      "Epoch 59: val_loss did not improve from 0.43389\n",
      "\n",
      "                                                                           \n",
      "Epoch 60: val_loss did not improve from 0.43389\n",
      "\n",
      "                                                                           \n",
      "Epoch 61: val_loss did not improve from 0.43389\n",
      "\n",
      "{'batch_size': 64, 'dropout_rate': 0.296538093813261, 'max_lr': 0.049953580245264785, 'n_hidden': 14, 'n_neurons': 32, 'rho': 0.8412810085725573}\n",
      "                                                                           \n",
      "Epoch 1: val_loss improved from inf to 1.69684, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 2: val_loss improved from 1.69684 to 1.12891, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 3: val_loss improved from 1.12891 to 0.87019, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 4: val_loss improved from 0.87019 to 0.80296, saving model to best_weights.h5\n",
      "\n",
      "                                                                           \n",
      "Epoch 5: val_loss did not improve from 0.80296\n",
      "\n",
      "                                                                           \n",
      "Epoch 6: val_loss did not improve from 0.80296\n",
      "\n",
      "                                                                           \n",
      "Epoch 7: val_loss did not improve from 0.80296\n",
      "\n",
      "                                                                           \n",
      "Epoch 8: val_loss did not improve from 0.80296\n",
      "\n",
      "                                                                           \n",
      "Epoch 9: val_loss did not improve from 0.80296\n",
      "\n",
      "                                                                           \n",
      "Epoch 10: val_loss did not improve from 0.80296\n",
      "\n",
      "                                                                           \n",
      "Epoch 11: val_loss did not improve from 0.80296\n",
      "\n",
      "                                                                           \n",
      "Epoch 12: val_loss did not improve from 0.80296\n",
      "\n",
      "                                                                           \n",
      "Epoch 13: val_loss did not improve from 0.80296\n",
      "\n",
      "                                                                           \n",
      "Epoch 14: val_loss did not improve from 0.80296\n",
      "\n",
      "                                                                           \n",
      "Epoch 15: val_loss did not improve from 0.80296\n",
      "\n",
      "                                                                           \n",
      "Epoch 16: val_loss did not improve from 0.80296\n",
      "\n",
      "                                                                           \n",
      "Epoch 17: val_loss did not improve from 0.80296\n",
      "\n",
      "                                                                           \n",
      "Epoch 18: val_loss did not improve from 0.80296\n",
      "\n",
      "                                                                           \n",
      "Epoch 19: val_loss did not improve from 0.80296\n",
      "\n",
      "                                                                           \n",
      "Epoch 20: val_loss did not improve from 0.80296\n",
      "\n",
      "                                                                           \n",
      "Epoch 21: val_loss did not improve from 0.80296\n",
      "\n",
      "                                                                           \n",
      "Epoch 22: val_loss did not improve from 0.80296\n",
      "\n",
      "                                                                           \n",
      "Epoch 23: val_loss did not improve from 0.80296\n",
      "\n",
      "                                                                           \n",
      "Epoch 24: val_loss did not improve from 0.80296\n",
      "\n",
      " 35%|  | 16/46 [01:09<02:10,  4.37s/trial, best loss: 0.46809840202331543]\n"
     ]
    }
   ],
   "source": [
    "search_space = {\n",
    "    'n_hidden': hp.choice('n_hidden', [4, 9, 14]),  # Hyperparameter space for the number of hidden layers\n",
    "    'n_neurons': hp.choice('n_neurons', [32, 64, 128]),  # Hyperparameter space for the number of neurons per layer\n",
    "    'max_lr': hp.loguniform('max_lr', np.log(0.03), np.log(0.05)),  # Hyperparameter space for the max learning rate (1cycle)\n",
    "    'rho': hp.loguniform('rho', np.log(0.7), np.log(0.99)),  # Hyperparameter space for the momentum\n",
    "    'dropout_rate': hp.uniform('dropout_rate', 0.1, 0.4),  # Hyperparameter space for the dropout rate\n",
    "    'batch_size': hp.choice('batch_size', [32, 48, 64])  # Hyperparameter space for the batch size\n",
    "}\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective, space=search_space, algo=tpe.suggest, max_evals=46, \n",
    "            early_stop_fn=no_progress_loss(8), trials=trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'batch_size': [2], 'dropout_rate': [0.28181808624951654], 'max_lr': [0.04537608465903134], 'n_hidden': [1], 'n_neurons': [0], 'rho': [0.8740009000916202]}\n",
      "Best Loss: 0.46809840202331543\n"
     ]
    }
   ],
   "source": [
    "best_trial = trials.best_trial\n",
    "best_params = best_trial[\"misc\"][\"vals\"]\n",
    "best_loss = best_trial[\"result\"][\"loss\"]\n",
    "\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best Loss:\", best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 64,\n",
       " 'dropout_rate': 0.28181808624951654,\n",
       " 'max_lr': 0.04537608465903134,\n",
       " 'n_hidden': 9,\n",
       " 'n_neurons': 32,\n",
       " 'rho': 0.8740009000916202}"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_best_params = space_eval(search_space, best)\n",
    "final_best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 0.46809840202331543,\n",
       " 'train_loss': 0.5117916464805603,\n",
       " 'status': 'ok',\n",
       " 'model': <keras.src.engine.sequential.Sequential at 0x212276b7730>,\n",
       " 'history': <keras.src.callbacks.History at 0x212794167d0>}"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_trial[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epoch_best = np.argmax(best_trial[\"result\"]['history'].history['val_accuracy']) + 1\n",
    "n_epoch_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dropout_rate': 0.28181808624951654,\n",
       " 'n_hidden': 9,\n",
       " 'n_neurons': 32,\n",
       " 'rho': 0.8740009000916202}"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_final_best_params = final_best_params.copy()\n",
    "mod_final_best_params.pop('max_lr')\n",
    "mod_final_best_params.pop('batch_size')\n",
    "mod_final_best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "1/2 [==============>...............] - ETA: 1s - loss: 1.4920 - accuracy: 0.3438\n",
      "Epoch 1: val_loss improved from inf to 0.61335, saving model to best_model_weights.h5\n",
      "2/2 [==============================] - 2s 362ms/step - loss: 1.5646 - accuracy: 0.3021 - val_loss: 0.6134 - val_accuracy: 0.5417\n",
      "Epoch 2/12\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.5541 - accuracy: 0.2031\n",
      "Epoch 2: val_loss did not improve from 0.61335\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 1.4326 - accuracy: 0.2708 - val_loss: 1.0243 - val_accuracy: 0.5417\n",
      "Epoch 3/12\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.1885 - accuracy: 0.4531\n",
      "Epoch 3: val_loss did not improve from 0.61335\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 1.1439 - accuracy: 0.4583 - val_loss: 1.2937 - val_accuracy: 0.5417\n",
      "Epoch 4/12\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 1.2312 - accuracy: 0.4219\n",
      "Epoch 4: val_loss did not improve from 0.61335\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 1.1785 - accuracy: 0.4896 - val_loss: 0.9411 - val_accuracy: 0.5833\n"
     ]
    }
   ],
   "source": [
    "# Train a model with the best hyperparameters\n",
    "n_epochs=12\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=3, \n",
    "                               restore_best_weights = False)  # Define early stopping callback\n",
    "    \n",
    "onecycle = OneCycleScheduler(math.ceil(len(X_train) / final_best_params['batch_size']) * n_epochs, \n",
    "                             max_rate=final_best_params['max_lr'])\n",
    "    \n",
    "best_model_file = \"best_model_weights.h5\" # File to save the best weights\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(best_model_file, monitor=\"val_loss\", mode=\"min\", \n",
    "                                                    save_best_only=True, verbose=1) # Define checkpoint to save best weights\n",
    "\n",
    "\n",
    "best_model_use_val = build_model(**mod_final_best_params)\n",
    "history__use_val = best_model_use_val.fit(X_train_prep, y_train_pre_transf, epochs=n_epochs, batch_size=final_best_params['batch_size'], \n",
    "                         callbacks=[early_stop, checkpoint, onecycle], validation_data=(X_val_prep, y_val), verbose=1)\n",
    "\n",
    "best_model_use_val.load_weights(best_model_file) # reload the best weights saved by checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/11\n",
      "2/2 [==============================] - 2s 6ms/step - loss: 1.5892 - accuracy: 0.3021\n",
      "Epoch 2/11\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.5657 - accuracy: 0.3542\n",
      "Epoch 3/11\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.5618 - accuracy: 0.3125\n",
      "Epoch 4/11\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.4453 - accuracy: 0.3438\n",
      "Epoch 5/11\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.0975 - accuracy: 0.4583\n",
      "Epoch 6/11\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.9124 - accuracy: 0.5625\n",
      "Epoch 7/11\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.8282 - accuracy: 0.6042\n",
      "Epoch 8/11\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.0910 - accuracy: 0.5625\n",
      "Epoch 9/11\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.0434 - accuracy: 0.4896\n",
      "Epoch 10/11\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.9928 - accuracy: 0.5625\n",
      "Epoch 11/11\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.8224 - accuracy: 0.5312\n"
     ]
    }
   ],
   "source": [
    "# Train another model with a full train set (train+val) using n_epoch_best instead of validation and early_stop\n",
    "\n",
    "X_train_full = np.concatenate((X_train_prep, X_val_prep), axis=0)\n",
    "y_train_full = np.concatenate((y_train_pre_transf, y_val), axis=0)\n",
    "\n",
    "onecycle = OneCycleScheduler(math.ceil(len(X_train) / final_best_params['batch_size']) * n_epochs, \n",
    "                             max_rate=final_best_params['max_lr'])\n",
    "\n",
    "best_model_epoch = build_model(**mod_final_best_params)\n",
    "history_epoch = best_model_epoch.fit(X_train_prep, y_train_pre_transf, epochs=n_epoch_best, \n",
    "                                     batch_size=final_best_params['batch_size'], callbacks=[onecycle], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use model saved from hyperopt direcly\n",
    "\n",
    "best_model_saved = best_trial[\"result\"]['model']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final evalution using Final_test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Prepare Final_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_test_set = Final_test_set_original.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>species</th>\n",
       "      <th>Label_1</th>\n",
       "      <th>Label_2</th>\n",
       "      <th>Label_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>D</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.9</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "15                5.0               3.3                1.4               0.2   \n",
       "16                5.7               2.8                4.1               1.3   \n",
       "17                4.9               3.6                1.4               0.1   \n",
       "18                6.7               3.3                5.7               2.1   \n",
       "19                6.2               3.4                5.4               2.3   \n",
       "20                5.0               2.3                3.3               1.0   \n",
       "21                7.7               3.8                6.7               2.2   \n",
       "22                6.4               2.8                5.6               2.1   \n",
       "23                5.0               3.0                1.6               0.2   \n",
       "24                4.7               3.2                1.6               0.2   \n",
       "25                5.1               3.8                1.5               0.3   \n",
       "26                5.6               3.0                4.1               1.3   \n",
       "27                5.1               3.8                1.6               0.2   \n",
       "28                5.6               2.9                3.6               1.3   \n",
       "29                4.9               3.1                1.5               0.2   \n",
       "\n",
       "    species Label_1 Label_2 Label_3  \n",
       "15        0       D       D       C  \n",
       "16        1       B       A       D  \n",
       "17        0       B       D       B  \n",
       "18        2       A       A       C  \n",
       "19        2       D       D       B  \n",
       "20        1       B       C       C  \n",
       "21        2       A       A       B  \n",
       "22        2       B       C       D  \n",
       "23        0       C       C       B  \n",
       "24        0       D       A       C  \n",
       "25        0       B       A       B  \n",
       "26        1       A       D       B  \n",
       "27        0       A       A       D  \n",
       "28        1       A       C       C  \n",
       "29        0       D       A       D  "
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Final_test_set.tail(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Make prediction with the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find if there is any column that is not numeric \n",
    "Final_test_set_cat = Final_test_set.select_dtypes(exclude=[np.number])\n",
    "#Final_test_set_cat.head()\n",
    "\n",
    "# Add categorical columns that are number manually can convert them to categorical type\n",
    "Final_test_set[other_cat_col] = Final_test_set[other_cat_col].astype(\"category\")\n",
    "#Final_test_set[other_cat_col].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_test_set_cat_encoded = ordinal_encoder.fit_transform(Final_test_set_cat)\n",
    "#print(Final_test_set_cat_encoded)\n",
    "\n",
    "Final_test_set[Final_test_set_cat.columns.tolist()] = Final_test_set_cat_encoded\n",
    "Final_test_set[Final_test_set_cat.columns.tolist()] = Final_test_set[Final_test_set_cat.columns.tolist()].astype(\"category\")\n",
    "#Final_test_set[Final_test_set_cat.columns.tolist()].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_test = Final_test_set.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    13\n",
       "2    10\n",
       "1     7\n",
       "Name: species, dtype: int64"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Final_test[\"species\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Final_test = Final_test.drop(\"species\", axis=1)\n",
    "y_Final_test = Final_test[\"species\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of components:  7\n",
      "shape after transformation:  (30, 7)\n"
     ]
    }
   ],
   "source": [
    "#Transform the X_test as the train set has been transformed\n",
    "X_Final_test_prep = full_pl.transform(X_Final_test)\n",
    "#print('shape before PCA: ', X_train_prep.shape)\n",
    "\n",
    "if use_PCA:\n",
    "    X_Final_test_prep = pca.transform(X_Final_test_prep)\n",
    "    print('number of components: ', pca.n_components_)\n",
    "\n",
    "print('shape after transformation: ', X_Final_test_prep.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n"
     ]
    }
   ],
   "source": [
    "# Use 3 models to make prediction\n",
    "\n",
    "y_pred_use_val_prob = best_model_use_val.predict(X_Final_test_prep) # use own validation\n",
    "y_pred_epoch_prob = best_model_epoch.predict(X_Final_test_prep) # use known best n_epochs\n",
    "y_pred_saved_prob = best_model_saved.predict(X_Final_test_prep) # use the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_probabilities_to_labels(predictions):\n",
    "    labels = np.argmax(predictions, axis=1)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_use_val_label = convert_probabilities_to_labels(y_pred_use_val_prob)\n",
    "y_pred_epoch_label = convert_probabilities_to_labels(y_pred_epoch_prob)\n",
    "y_pred_saved_label = convert_probabilities_to_labels(y_pred_saved_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Evaluate the model with test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perf_evaluate_plot(y_test, y_pred, model = 1):\n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Define class names\n",
    "    if model == 1:\n",
    "        classes = ['Class 0', 'Class 1']\n",
    "    elif model == 2:\n",
    "        classes = ['Class 1', 'Class 2']\n",
    "    elif model == 3:\n",
    "        classes = ['Class 0', 'Class 1', 'Class 2']\n",
    "    else:\n",
    "        sys.exit(\"n_class can only be 2 or 3\")\n",
    "    \n",
    "    # Define plot parameters\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.heatmap(cm, annot=True, cmap=\"YlGnBu\", fmt=\"d\", xticklabels=classes, yticklabels=classes, annot_kws={\"fontsize\":16})\n",
    "    \n",
    "    # Set plot labels\n",
    "    ax.set_xlabel('Predicted label')\n",
    "    ax.set_ylabel('True label')\n",
    "    ax.set_title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    # Create classification report\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"====================================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Own validation net performance evaluation\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        13\n",
      "           1       0.40      0.86      0.55         7\n",
      "           2       0.50      0.10      0.17        10\n",
      "\n",
      "    accuracy                           0.67        30\n",
      "   macro avg       0.63      0.65      0.57        30\n",
      "weighted avg       0.69      0.67      0.62        30\n",
      "\n",
      "====================================================================================\n",
      "n_epoch Model net performance evaluation\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      1.00      0.87        13\n",
      "           1       0.00      0.00      0.00         7\n",
      "           2       0.77      1.00      0.87        10\n",
      "\n",
      "    accuracy                           0.77        30\n",
      "   macro avg       0.51      0.67      0.58        30\n",
      "weighted avg       0.59      0.77      0.67        30\n",
      "\n",
      "====================================================================================\n",
      "Saved Model performance evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chonl\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\chonl\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\chonl\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        13\n",
      "           1       1.00      0.71      0.83         7\n",
      "           2       0.83      1.00      0.91        10\n",
      "\n",
      "    accuracy                           0.93        30\n",
      "   macro avg       0.94      0.90      0.91        30\n",
      "weighted avg       0.94      0.93      0.93        30\n",
      "\n",
      "====================================================================================\n"
     ]
    }
   ],
   "source": [
    "# use_val performance\n",
    "print(\"Own validation net performance evaluation\")\n",
    "perf_evaluate_plot(Final_test[\"species\"], y_pred_use_val_label, model=3)\n",
    "\n",
    "# best n_epoch performance\n",
    "print(\"n_epoch Model net performance evaluation\")\n",
    "perf_evaluate_plot(Final_test[\"species\"], y_pred_epoch_label, model=3)\n",
    "\n",
    "# Saved model performance\n",
    "print(\"Saved Model performance evaluation\")\n",
    "perf_evaluate_plot(Final_test[\"species\"], y_pred_saved_label, model=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final words\n",
    "\n",
    "The result based on the saved model is pretty good while. However, the two-level DL model performed better than this multiclass version. It was actually a bit of a surprise becuase this model was the best-performing model when tested on the actual dataset of the problem that it was built to solve (not IRIS). Out of the three final models, the best model automatically saved from the training phase is much much better that the retrained model, while their performances were comparable on the actual dataset. This highlights the fact that machine learning can be specific and customised solution is usually needed to treat each problem.\n",
    "\n",
    "Anyways, the idea of this notebook is to demonstrate how to use deep learning neural network to perform classification problem when the model has two-level architecture. It is possible to compare the performance of the classifier of each level to the global performance to observe the performance drop caused by error propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
